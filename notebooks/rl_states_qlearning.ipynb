{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qJX3Ez_7kg_"
      },
      "source": [
        "# Main Notebook to run the Code in Colab\n",
        "\n",
        "Open this notebook in Colab, the first few cells will copy the repo from git and make the modules available in the notebook itself.\n",
        "\n",
        "Import any packages from the GitHub repo (for example from the src directory)\n",
        "by calling src.package_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IR4xkiI7khB",
        "outputId": "5f33a69f-306f-4c90-cbea-188ca70ce470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MLRFH' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# only clone and run this notebook in colab. The import should fail otherwise\n",
        "import google.colab\n",
        "from google.colab import files\n",
        "import os\n",
        "import sys\n",
        "\n",
        "!git clone https://github.com/AveraGE0/MLRFH.git\n",
        "\n",
        "src_path = os.path.join(\"MLRFH/\")\n",
        "# Add src folder to sys.path if not already added\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxBtrdwI76Yb",
        "outputId": "11b5faf1-f8d1-4f3e-91dc-ee83db638710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From https://github.com/AveraGE0/MLRFH\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "os.chdir(\"MLRFH/\")\n",
        "!git pull https://github.com/AveraGE0/MLRFH.git\n",
        "os.chdir(\"../\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z2QTA_rDZEC3"
      },
      "outputs": [],
      "source": [
        "\n",
        "PROJECT_ID = \"mrih-440308\" # Replace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THBD0ttmZN-m",
        "outputId": "86b9d89b-1dba-4f8d-cc68-170f4fd963d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "\n",
        "DATASET_PROJECT_ID = 'amsterdamumcdb'\n",
        "DATASET_ID = 'version1_5_0'\n",
        "LOCATION = 'eu'\n",
        "\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6-OCSgaKZha8"
      },
      "outputs": [],
      "source": [
        "%load_ext google.colab.data_table\n",
        "from google.colab.data_table import DataTable\n",
        "\n",
        "DataTable.max_columns = 30\n",
        "DataTable.max_rows = 30000\n",
        "\n",
        "from google.cloud.bigquery import magics\n",
        "from google.cloud import bigquery\n",
        "\n",
        "def_config = bigquery.job.QueryJobConfig(default_dataset=DATASET_PROJECT_ID + \".\" + DATASET_ID)\n",
        "magics.context.default_query_job_config = def_config\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "config_gbq = {\n",
        "    'query': {\n",
        "        'defaultDataset': {\n",
        "              \"datasetId\": DATASET_ID,\n",
        "              \"projectId\": DATASET_PROJECT_ID\n",
        "        },\n",
        "    'Location': LOCATION}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xi2OYmtAZmbV",
        "outputId": "e9dce0de-d09f-46af-a3a5-97562ce53954",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: amsterdamumcdb in /usr/local/lib/python3.11/dist-packages (0.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from amsterdamumcdb) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from amsterdamumcdb) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->amsterdamumcdb) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->amsterdamumcdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->amsterdamumcdb) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->amsterdamumcdb) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->amsterdamumcdb) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->amsterdamumcdb) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->amsterdamumcdb) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->amsterdamumcdb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->amsterdamumcdb) (1.17.0)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "#get the amsterdamumcdb package from PyPI repository for use in Colab\n",
        "!pip install amsterdamumcdb\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "import torch\n",
        "import amsterdamumcdb as adb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wberWtqt7gCR",
        "outputId": "ab294377-0098-494c-e985-50a8cd44354e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "INFO: \n",
            "Unique septic persons: 1746\n",
            "Unique septic admissions: 1882\n",
            "INFO: \n",
            "Unique septic shock persons: 1258\n",
            "Unique septic shock admissions: 1321\n",
            "Septic shock patients: 1321\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Unique features being loaded ['Weight' 'Heart Rate' 'Systolic Blood Pressure' 'Mean Blood Pressure'\n",
            " 'Diastolic blood pressure' 'Respiratory rate' 'SpO2' 'Temperature'\n",
            " 'Potassium' 'Sodium' 'Chloride' 'Glucose' 'Urea' 'Creatinine' 'Magnesium'\n",
            " 'Calcium.ionized' 'Calcium' 'paCO2' 'Aspartate aminotransferase'\n",
            " 'Alanine aminotransferase' 'Bilirubin.total' 'Albumin' 'Hemoglobin'\n",
            " 'Leukocytes' 'Platelets' 'aPTT' 'PT' 'INR' 'pH Blood' 'PaOxygen'\n",
            " 'Base excess' 'Bicarbonate' 'Lactate' 'FiO2' 'Urine']\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Cols : Index(['measurement_concept_id', 'concept_name', 'measurement_datetime',\n",
            "       'value_as_number', 'visit_occurrence_id'],\n",
            "      dtype='object')\n",
            "Measurements for temp: 488262\n",
            "Datapoints before: 37228303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "processing f35 features: 100%|██████████| 35/35 [00:54<00:00,  1.56s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 1257564 outliers.\n",
            "35970739 values are still present.\n",
            "Index before: Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64'), after: RangeIndex(start=0, stop=10, step=1)\n",
            "Wide temp dps: 461678\n",
            "Wide temp rows: 1126773\n",
            "feature_name  visit_occurrence_id      measurement_datetime  \\\n",
            "0                              11 2005-12-31 22:11:00+00:00   \n",
            "1                              11 2005-12-31 22:19:00+00:00   \n",
            "2                              11 2005-12-31 22:50:00+00:00   \n",
            "3                              11 2005-12-31 23:04:00+00:00   \n",
            "4                              11 2006-01-01 00:00:00+00:00   \n",
            "\n",
            "feature_name  Alanine aminotransferase  Albumin  Aspartate aminotransferase  \\\n",
            "0                                 54.0      NaN                        85.0   \n",
            "1                                  NaN      NaN                         NaN   \n",
            "2                                  NaN      NaN                         NaN   \n",
            "3                                  NaN      NaN                         NaN   \n",
            "4                                  NaN      NaN                         NaN   \n",
            "\n",
            "feature_name  Base excess  Bicarbonate  Bilirubin.total  Calcium  \\\n",
            "0                    -9.0          NaN              NaN      NaN   \n",
            "1                     NaN          NaN              NaN      NaN   \n",
            "2                    -7.9         14.2              NaN      NaN   \n",
            "3                    -9.0         17.0              NaN      NaN   \n",
            "4                     NaN          NaN              NaN      NaN   \n",
            "\n",
            "feature_name  Calcium.ionized  ...  Sodium  SpO2  Systolic Blood Pressure  \\\n",
            "0                         NaN  ...   131.0   NaN                      NaN   \n",
            "1                         NaN  ...     NaN   NaN                      NaN   \n",
            "2                         NaN  ...     NaN   NaN                      NaN   \n",
            "3                        0.94  ...   132.0   NaN                      NaN   \n",
            "4                         NaN  ...     NaN   NaN                      NaN   \n",
            "\n",
            "feature_name  Temperature  Urea  Urine  Weight  aPTT  pH Blood  paCO2  \n",
            "0                     NaN   NaN    NaN     NaN   NaN      7.48    NaN  \n",
            "1                     NaN   NaN    NaN     NaN  47.0       NaN    NaN  \n",
            "2                     NaN   NaN    NaN     NaN   NaN      7.42   23.0  \n",
            "3                     NaN   NaN    NaN     NaN   NaN      7.28   37.0  \n",
            "4                     NaN   NaN    NaN    85.0   NaN       NaN    NaN  \n",
            "\n",
            "[5 rows x 37 columns]\n",
            "Index(['visit_occurrence_id', 'measurement_datetime',\n",
            "       'Alanine aminotransferase', 'Albumin', 'Aspartate aminotransferase',\n",
            "       'Base excess', 'Bicarbonate', 'Bilirubin.total', 'Calcium',\n",
            "       'Calcium.ionized', 'Chloride', 'Creatinine', 'Diastolic blood pressure',\n",
            "       'FiO2', 'Glucose', 'Heart Rate', 'Hemoglobin', 'INR', 'Lactate',\n",
            "       'Leukocytes', 'Magnesium', 'Mean Blood Pressure', 'PT', 'PaOxygen',\n",
            "       'Platelets', 'Potassium', 'Respiratory rate', 'Sodium', 'SpO2',\n",
            "       'Systolic Blood Pressure', 'Temperature', 'Urea', 'Urine', 'Weight',\n",
            "       'aPTT', 'pH Blood', 'paCO2'],\n",
            "      dtype='object', name='feature_name')\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing 2 columns:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "procssing column ['pao2_fio2_ratio', 'pao2']\n",
            "procssing column ['pao2_fio2_ratio', 'pao2']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing 2 columns: 100%|██████████| 2/2 [00:00<00:00,  3.99it/s]\n",
            "100%|██████████| 1321/1321 [00:04<00:00, 285.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['visit_occurrence_id', 'measurement_datetime',\n",
            "       'Alanine aminotransferase', 'Albumin', 'Aspartate aminotransferase',\n",
            "       'Base excess', 'Bicarbonate', 'Bilirubin.total', 'Calcium',\n",
            "       'Calcium.ionized', 'Chloride', 'Creatinine', 'Diastolic blood pressure',\n",
            "       'FiO2', 'Glucose', 'Heart Rate', 'Hemoglobin', 'INR', 'Lactate',\n",
            "       'Leukocytes', 'Magnesium', 'Mean Blood Pressure', 'PT', 'PaOxygen',\n",
            "       'Platelets', 'Potassium', 'Respiratory rate', 'Sodium', 'SpO2',\n",
            "       'Systolic Blood Pressure', 'Temperature', 'Urea', 'Urine', 'Weight',\n",
            "       'aPTT', 'pH Blood', 'paCO2', 'sofa_cns_score', 'ventilatory_support',\n",
            "       'pao2_fio2_ratio', 'pao2', 'Gender', 'age_at_visit'],\n",
            "      dtype='object')\n",
            "dataset has 1464083 non-na entries!\n",
            "transforming Alanine aminotransferase\n",
            "transforming Albumin\n",
            "transforming Aspartate aminotransferase\n",
            "transforming Base excess\n",
            "transforming Bicarbonate\n",
            "transforming Bilirubin.total\n",
            "transforming Calcium\n",
            "transforming Calcium.ionized\n",
            "transforming Chloride\n",
            "transforming Creatinine\n",
            "transforming Diastolic blood pressure\n",
            "transforming FiO2\n",
            "transforming Glucose\n",
            "transforming Heart Rate\n",
            "transforming Hemoglobin\n",
            "transforming INR\n",
            "transforming Lactate\n",
            "transforming Leukocytes\n",
            "transforming Magnesium\n",
            "transforming Mean Blood Pressure\n",
            "transforming PT\n",
            "transforming PaOxygen\n",
            "transforming Platelets\n",
            "transforming Potassium\n",
            "transforming Respiratory rate\n",
            "transforming Sodium\n",
            "transforming SpO2\n",
            "transforming Systolic Blood Pressure\n",
            "transforming Temperature\n",
            "transforming Urea\n",
            "transforming Urine\n",
            "transforming Weight\n",
            "transforming aPTT\n",
            "transforming pH Blood\n",
            "transforming paCO2\n",
            "transforming sofa_cns_score\n",
            "transforming pao2_fio2_ratio\n",
            "transforming pao2\n",
            "transforming age_at_visit\n",
            "       Alanine aminotransferase       Albumin  Aspartate aminotransferase  \\\n",
            "count              1.119300e+04  2.081500e+04                1.106500e+04   \n",
            "mean              -1.777468e-16 -9.640032e-16                1.977832e-16   \n",
            "std                1.000000e+00  1.000000e+00                1.000000e+00   \n",
            "min               -3.555820e+00 -3.194652e+00               -3.421844e+00   \n",
            "25%               -7.265137e-01 -6.501578e-01               -7.291254e-01   \n",
            "50%               -3.066913e-02  4.078989e-02               -2.728389e-02   \n",
            "75%                7.313903e-01  6.842864e-01                7.236682e-01   \n",
            "max                2.099118e+00  2.409345e+00                2.125376e+00   \n",
            "\n",
            "        Base excess   Bicarbonate  Bilirubin.total       Calcium  \\\n",
            "count  4.659600e+04  4.656700e+04     1.090500e+04  2.490100e+04   \n",
            "mean   1.250419e-17  9.338204e-17     1.570296e-16  2.784987e-16   \n",
            "std    1.000000e+00  1.000000e+00     1.000000e+00  1.000000e+00   \n",
            "min   -2.608503e+00 -2.793942e+00    -2.957541e+00 -2.866665e+00   \n",
            "25%   -6.703961e-01 -6.555090e-01    -7.942452e-01 -6.346514e-01   \n",
            "50%    1.955926e-02  1.303899e-02     2.120672e-02 -8.572789e-03   \n",
            "75%    6.678552e-01  6.643769e-01     7.619972e-01  6.064747e-01   \n",
            "max    3.220500e+00  2.854867e+00     2.027653e+00  2.680515e+00   \n",
            "\n",
            "       Calcium.ionized      Chloride    Creatinine  ...        Weight  \\\n",
            "count     3.550500e+04  3.590600e+04  3.164900e+04  ...  1.256000e+03   \n",
            "mean     -8.921559e-16 -1.931130e-12  2.595303e-16  ... -1.533098e-15   \n",
            "std       1.000000e+00  1.000000e+00  1.000000e+00  ...  1.000000e+00   \n",
            "min      -2.532077e+00 -3.814273e+00 -6.317582e+00  ... -1.759215e+00   \n",
            "25%      -6.313219e-01 -7.585294e-01 -7.320043e-01  ... -8.927195e-01   \n",
            "50%       6.092424e-02 -3.028823e-02 -6.253272e-02  ... -1.593280e-01   \n",
            "75%       6.294556e-01  7.497519e-01  7.315590e-01  ...  4.753138e-01   \n",
            "max       3.024157e+00  2.360145e+00  2.114402e+00  ...  1.981400e+00   \n",
            "\n",
            "               aPTT      pH Blood         paCO2  sofa_cns_score  \\\n",
            "count  2.709100e+04  4.667500e+04  4.642300e+04    2.964000e+04   \n",
            "mean   3.570155e-15 -7.428920e-17 -1.562420e-15   -2.378065e-16   \n",
            "std    1.000000e+00  1.000000e+00  1.000000e+00    1.000000e+00   \n",
            "min   -9.981000e+00 -2.440421e+00 -4.131664e+00   -1.159455e+00   \n",
            "25%   -8.013139e-01 -7.105881e-01 -6.664816e-01   -1.159455e+00   \n",
            "50%   -7.010253e-02  6.223468e-02 -3.691128e-02    8.624454e-01   \n",
            "75%    7.928049e-01  7.442659e-01  6.532680e-01    8.624454e-01   \n",
            "max    2.349587e+00  4.336550e+00  2.552962e+00    8.624454e-01   \n",
            "\n",
            "       ventilatory_support  pao2_fio2_ratio          pao2        Gender  \\\n",
            "count              43506.0     4.350600e+04  4.350600e+04  53304.000000   \n",
            "mean              0.732286     3.547324e-16  1.081183e-16      0.609429   \n",
            "std               0.422276     1.000000e+00  1.000000e+00      0.487883   \n",
            "min                    0.0    -2.007454e+00 -8.796453e+00      0.000000   \n",
            "25%               0.333333    -7.264766e-01 -6.373882e-01      0.000000   \n",
            "50%                    1.0    -5.090119e-01 -6.922187e-02      1.000000   \n",
            "75%                    1.0     1.425733e+00  5.954480e-01      1.000000   \n",
            "max                    1.0     1.597858e+00  2.232691e+01      1.000000   \n",
            "\n",
            "       age_at_visit  \n",
            "count  5.483600e+04  \n",
            "mean   1.119536e-16  \n",
            "std    1.000000e+00  \n",
            "min   -1.871053e+00  \n",
            "25%   -7.341640e-01  \n",
            "50%   -4.850857e-02  \n",
            "75%    7.771832e-01  \n",
            "max    2.288061e+00  \n",
            "\n",
            "[8 rows x 41 columns]\n",
            "Percentage of Missing Data Per Feature:\n",
            "Alanine aminotransferase      82.325354\n",
            "Albumin                       67.131443\n",
            "Aspartate aminotransferase    82.527476\n",
            "Base excess                   26.421172\n",
            "Bicarbonate                   26.466966\n",
            "Bilirubin.total               82.780129\n",
            "Calcium                       60.679320\n",
            "Calcium.ionized               43.934752\n",
            "Chloride                      43.301541\n",
            "Creatinine                    50.023686\n",
            "Diastolic blood pressure      29.146665\n",
            "FiO2                          50.301604\n",
            "Glucose                       20.602261\n",
            "Heart Rate                    99.425215\n",
            "Hemoglobin                    26.056405\n",
            "INR                           61.400960\n",
            "Lactate                       72.757706\n",
            "Leukocytes                    50.604788\n",
            "Magnesium                     67.085649\n",
            "Mean Blood Pressure           29.148244\n",
            "PT                            98.255116\n",
            "PaOxygen                      27.160182\n",
            "Platelets                     50.192648\n",
            "Potassium                     22.877716\n",
            "Respiratory rate              65.767117\n",
            "Sodium                        21.543393\n",
            "SpO2                          25.942711\n",
            "Systolic Blood Pressure       29.159298\n",
            "Temperature                   93.568406\n",
            "Urea                          67.966776\n",
            "Urine                         28.976124\n",
            "Weight                        98.016675\n",
            "aPTT                          57.221134\n",
            "pH Blood                      26.296425\n",
            "paCO2                         26.694353\n",
            "sofa_cns_score                53.196059\n",
            "ventilatory_support           31.300531\n",
            "pao2_fio2_ratio               31.300531\n",
            "pao2                          31.300531\n",
            "Gender                        15.828701\n",
            "age_at_visit                  13.409550\n",
            "dtype: float64\n",
            "Percentage of visit occurrences retained: 75.98%\n",
            "Number of visit occurrences after filtering: 48117\n",
            "Dropped the following columns: MultiIndex([(   37, '2013-01-06 12:00:00+00:00'),\n",
            "            (   38, '2013-02-05 00:00:00+00:00'),\n",
            "            (   47, '2012-12-30 12:00:00+00:00'),\n",
            "            (   47, '2012-12-31 00:00:00+00:00'),\n",
            "            (   47, '2013-01-07 12:00:00+00:00'),\n",
            "            (   50, '2005-12-31 12:00:00+00:00'),\n",
            "            (   50, '2006-02-06 12:00:00+00:00'),\n",
            "            (   66, '2005-12-19 00:00:00+00:00'),\n",
            "            (   66, '2005-12-19 12:00:00+00:00'),\n",
            "            (   66, '2005-12-20 00:00:00+00:00'),\n",
            "            ...\n",
            "            (23483, '2013-05-11 00:00:00+00:00'),\n",
            "            (23483, '2013-05-11 12:00:00+00:00'),\n",
            "            (23483, '2013-05-12 12:00:00+00:00'),\n",
            "            (23483, '2013-05-13 12:00:00+00:00'),\n",
            "            (23540, '2012-12-29 00:00:00+00:00'),\n",
            "            (23540, '2012-12-29 12:00:00+00:00'),\n",
            "            (23540, '2012-12-30 00:00:00+00:00'),\n",
            "            (23540, '2012-12-30 12:00:00+00:00'),\n",
            "            (23540, '2012-12-31 00:00:00+00:00'),\n",
            "            (23540, '2013-01-03 12:00:00+00:00')],\n",
            "           names=['visit_occurrence_id', 'measurement_datetime'], length=15211)\n",
            "Percentage of Missing Data Per Feature:\n",
            "Alanine aminotransferase      38.078018\n",
            "Albumin                       25.747657\n",
            "Aspartate aminotransferase    37.321529\n",
            "Base excess                    2.805661\n",
            "Bicarbonate                    2.913731\n",
            "Bilirubin.total               39.295883\n",
            "Calcium                       20.861650\n",
            "Calcium.ionized               22.877569\n",
            "Chloride                      20.743188\n",
            "Creatinine                     5.062660\n",
            "Diastolic blood pressure       9.032151\n",
            "FiO2                          32.221460\n",
            "Glucose                        0.835464\n",
            "Heart Rate                    95.643951\n",
            "Hemoglobin                     1.662614\n",
            "INR                           29.781574\n",
            "Lactate                       53.241058\n",
            "Leukocytes                     4.027683\n",
            "Magnesium                     37.045119\n",
            "Mean Blood Pressure            9.032151\n",
            "PT                            94.482200\n",
            "PaOxygen                       3.269115\n",
            "Platelets                      4.139909\n",
            "Potassium                      1.034977\n",
            "Respiratory rate              53.783486\n",
            "Sodium                         0.241079\n",
            "SpO2                           7.677120\n",
            "Systolic Blood Pressure        9.025916\n",
            "Temperature                   78.947150\n",
            "Urea                          40.314234\n",
            "Urine                         10.601243\n",
            "Weight                        12.101752\n",
            "aPTT                          23.912547\n",
            "pH Blood                       2.707983\n",
            "paCO2                          3.059210\n",
            "sofa_cns_score                47.845876\n",
            "ventilatory_support            8.487645\n",
            "pao2_fio2_ratio                8.487645\n",
            "pao2                           8.487645\n",
            "Gender                         2.946983\n",
            "age_at_visit                   0.000000\n",
            "dtype: float64\n",
            "Train data shape: (30909, 41)\n",
            "Validation data shape: (8290, 41)\n",
            "Test data shape: (8918, 41)\n",
            "Starting k means imputation\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from src.data_processing.process import process_data\n",
        "\n",
        "\n",
        "df_train, df_val, df_test = process_data(PROJECT_ID, config_gbq, default_path=\"./MLRFH\")\n",
        "\n",
        "#df_train[\"pao2\"] = (df_train[\"pao2\"] - df_train[\"pao2\"].mean()) / df_train[\"pao2\"].std()\n",
        "df_train = df_train.reset_index()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df_test.reset_index()"
      ],
      "metadata": {
        "id": "sIPD1oibKSxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.drop(columns=[\"Bicarbonate\"])\n",
        "df_val = df_val.drop(columns=[\"Bicarbonate\"])\n",
        "df_test = df_test.drop(columns=[\"Bicarbonate\"])"
      ],
      "metadata": {
        "id": "PNuPtN_e8Un9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwcf-Bqf8OaR"
      },
      "source": [
        "# Making the Actions (Space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqmLBfDo8TaK"
      },
      "source": [
        "## Vasopressors and Ionotropes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oK9CcKS8cG2"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "WITH filtered_measurement AS (\n",
        "    SELECT\n",
        "        visit_occurrence_id,\n",
        "        value_as_number AS patientweight\n",
        "    FROM measurement\n",
        "    WHERE provider_id IS NULL\n",
        "    AND measurement_concept_id IN (\n",
        "        3026600, -- Body weight Estimated\n",
        "        3013762, -- Body weight Measured\n",
        "        3023166, -- Body weight Stated\n",
        "        3025315  -- Body weight\n",
        "    )\n",
        "),\n",
        "dosing AS (\n",
        "    SELECT\n",
        "        de.person_id,\n",
        "        de.visit_occurrence_id,\n",
        "        de.drug_concept_id AS itemid,\n",
        "        c.concept_name AS item,\n",
        "        de.drug_exposure_start_datetime AS start_datetime,\n",
        "        de.drug_exposure_end_datetime AS stop_datetime,\n",
        "        TIMESTAMP_DIFF(de.drug_exposure_end_datetime, de.drug_exposure_start_datetime, MINUTE) AS duration,\n",
        "        -- Extract dose and rate from the sig field\n",
        "        CAST(REGEXP_EXTRACT(de.sig, r'(\\\\d+\\\\.?\\\\d*) mg') AS FLOAT64) AS dose,\n",
        "        CAST(REGEXP_EXTRACT(de.sig, r'@ (\\\\d+\\\\.?\\\\d*) mg/uur') AS FLOAT64) AS rate,\n",
        "        'mg/uur' AS rateunit,\n",
        "        fm.patientweight\n",
        "    FROM drug_exposure de\n",
        "    LEFT JOIN visit_occurrence vo ON de.visit_occurrence_id = vo.visit_occurrence_id\n",
        "    LEFT JOIN concept c ON de.drug_concept_id = c.concept_id\n",
        "    LEFT JOIN filtered_measurement fm ON de.visit_occurrence_id = fm.visit_occurrence_id\n",
        "    WHERE c.concept_id IN (\n",
        "            36411287, -- 50 ML Dopamine 4 MG/ML Injectable Solution\n",
        "            21088391, -- 50 ML Dobutamine 5 MG/ML Injection\n",
        "            19076867, -- Epinephrine 0.1 MG/ML Injectable Solution\n",
        "            2907531  -- 50 ML Norepinephrine 0.2 MG/ML Injection\n",
        "        )\n",
        "    AND de.visit_occurrence_id IN {visit_occurrence_ids}\n",
        "    AND CAST(REGEXP_EXTRACT(de.sig, r'@ (\\\\d+\\\\.?\\\\d*) mg/uur') AS FLOAT64) > 0.1\n",
        ")\n",
        "SELECT\n",
        "    person_id,\n",
        "    visit_occurrence_id,\n",
        "    itemid,\n",
        "    item,\n",
        "    duration,\n",
        "    dose,\n",
        "    rate,\n",
        "    rateunit,\n",
        "    start_datetime,\n",
        "    stop_datetime,\n",
        "    patientweight,\n",
        "    CASE\n",
        "        -- recalculate the dose to µg/kg/min ('gamma')\n",
        "        WHEN rateunit = 'mg/uur' THEN (rate * 1000) / patientweight / 60 -- convert mg/hour to µg/kg/min\n",
        "        ELSE NULL -- Placeholder for other conversions if necessary\n",
        "    END AS gamma\n",
        "FROM dosing\n",
        "ORDER BY visit_occurrence_id, start_datetime\n",
        "\"\"\"\n",
        "\n",
        "sepsis_vasopressors_train = pd.read_gbq(query.format(visit_occurrence_ids=tuple(df_train.reset_index()[\"visit_occurrence_id\"].tolist())), configuration=config_gbq, use_bqstorage_api=True)\n",
        "\n",
        "#print(sepsis_vasopressors.head())\n",
        "print(f\"Etracted {len(sepsis_vasopressors_train)} records\")\n",
        "print(f\"Unique vasopressors {sepsis_vasopressors_train['item'].unique()}\")\n",
        "\n",
        "sepsis_vasopressors_train.head()\n",
        "\n",
        "sepsis_vasopressors_test = pd.read_gbq(query.format(visit_occurrence_ids=tuple(df_test.reset_index()[\"visit_occurrence_id\"].tolist())), configuration=config_gbq, use_bqstorage_api=True)\n",
        "\n",
        "#print(sepsis_vasopressors.head())\n",
        "print(f\"Etracted {len(sepsis_vasopressors_test)} records\")\n",
        "print(f\"Unique vasopressors {sepsis_vasopressors_test['item'].unique()}\")\n",
        "\n",
        "sepsis_vasopressors_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUjrwH6eLD_w"
      },
      "source": [
        "### Calculating Epinephrine Equivalent Values[[1](https://pmc.ncbi.nlm.nih.gov/articles/PMC3590882/pdf/chest_143_3_664.pdf)] (e-table 2)\n",
        "\n",
        "| Vasopressor | Norepinephrine Equivalent Dose |\n",
        "|-|-|\n",
        "|Epinephrine| 1 |\n",
        "| Norepinephrin | 1 |\n",
        "|Dopamin| 0.01|\n",
        "|Dobutamine**| 0.01 |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "** Derived from VIS-score[[1](https://www.sciencedirect.com/science/article/abs/pii/S1053077020310351?fr=RR-1&ref=cra_js_challenge)]\n",
        "\n",
        "Exact values might differ [[2](https://www.researchgate.net/publication/367302762_An_updated_norepinephrine_equivalent_score_in_intensive_care_as_a_marker_of_shock_severity/fulltext/63cb3720d9fb5967c2f1bafe/An-updated-norepinephrine-equivalent-score-in-intensive-care-as-a-marker-of-shock-severity.pdf?origin=publication_detail&_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uRG93bmxvYWQiLCJwcmV2aW91c1BhZ2UiOiJwdWJsaWNhdGlvbiJ9fQ)]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXvkUxA8U1if"
      },
      "outputs": [],
      "source": [
        "convert_dict = {\n",
        "    \"epinephrine\": 1.0,\n",
        "    \"norepinephrine\": 1.0,\n",
        "    \"dopamine\": 0.01,\n",
        "    \"dobutamine\": 0.01,\n",
        "}\n",
        "\n",
        "\n",
        "def equalize_dose(row):\n",
        "    for key in convert_dict.keys():\n",
        "        if key in row[\"item\"].lower():\n",
        "            return row[\"gamma\"] * convert_dict[key]\n",
        "    raise ValueError(f\"Unknown vasopressor {row['item']}\")\n",
        "\n",
        "\n",
        "sepsis_vasopressors_train[\"equiv_gamma\"] = sepsis_vasopressors_train.apply(equalize_dose, axis=1)\n",
        "\n",
        "# drop outliers based in inter quantile range\n",
        "sepsis_vasopressors_cleaned_train = sepsis_vasopressors_train[sepsis_vasopressors_train[\"equiv_gamma\"] < sepsis_vasopressors_train[\"equiv_gamma\"].quantile(0.99)]\n",
        "sepsis_vasopressors_cleaned_train = sepsis_vasopressors_cleaned_train.reset_index()\n",
        "\n",
        "for item in sepsis_vasopressors_cleaned_train[\"item\"].unique():\n",
        "    print(\"Mean dose for:\", item, sepsis_vasopressors_cleaned_train[sepsis_vasopressors_cleaned_train[\"item\"] == item][\"equiv_gamma\"].mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sepsis_vasopressors_test[\"equiv_gamma\"] = sepsis_vasopressors_test.apply(equalize_dose, axis=1)\n",
        "\n",
        "# drop outliers based in inter quantile range\n",
        "sepsis_vasopressors_cleaned_test = sepsis_vasopressors_test[sepsis_vasopressors_test[\"equiv_gamma\"] < sepsis_vasopressors_test[\"equiv_gamma\"].quantile(0.99)]\n",
        "sepsis_vasopressors_cleaned_test = sepsis_vasopressors_cleaned_test.reset_index()\n",
        "\n",
        "for item in sepsis_vasopressors_cleaned_test[\"item\"].unique():\n",
        "    print(\"Mean dose for:\", item, sepsis_vasopressors_cleaned_test[sepsis_vasopressors_cleaned_test[\"item\"] == item][\"equiv_gamma\"].mean())"
      ],
      "metadata": {
        "id": "rdwZdd5vH9qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_tUOHKUVwiM"
      },
      "outputs": [],
      "source": [
        "# make histogram of data\n",
        "sepsis_vasopressors_cleaned_train[\"equiv_gamma\"].hist(bins=100)\n",
        "\n",
        "bins = [0, 0, 0.08, 0.22, 0.45]\n",
        "\n",
        "\n",
        "print(\"|Dose Range\\t|Mean|Mode|% of Data\")\n",
        "for i, bin in enumerate(bins):\n",
        "    if i == 0:\n",
        "        print(f\"|[0]|?\\t|?\\t|?\\t\")\n",
        "        continue\n",
        "    if i == len(bins) - 1:\n",
        "        bin_vasopressor = sepsis_vasopressors_cleaned_train[\n",
        "              sepsis_vasopressors_cleaned_train[\"equiv_gamma\"] > bin\n",
        "        ]\n",
        "        print(f\"|>{bin}\", end=\"\")\n",
        "    else:\n",
        "        bin_vasopressor = sepsis_vasopressors_cleaned_train[\n",
        "            (sepsis_vasopressors_cleaned_train[\"equiv_gamma\"] > bin) & (sepsis_vasopressors_cleaned_train[\"equiv_gamma\"] <= bins[i+1])\n",
        "        ]\n",
        "        print(f\"|]{bin}-{bins[i+1]}\", end=\"\")\n",
        "    print(f\"|{bin_vasopressor['equiv_gamma'].mean()}|{bin_vasopressor['equiv_gamma'].mode().mean()}|{len(bin_vasopressor)/len(sepsis_vasopressors_cleaned_train)}\")\n",
        "    plt.axvline(bin, color=\"red\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1glP5ivt8Xfe"
      },
      "source": [
        "## Fluid Intake"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fluid_intake_query = '''\n",
        "    SELECT m.person_id, m.measurement_concept_id, c.concept_name, measurement_datetime, value_as_number, m.visit_occurrence_id\n",
        "    FROM measurement as m\n",
        "\n",
        "    LEFT JOIN concept c ON m.measurement_concept_id = c.concept_id\n",
        "\n",
        "    WHERE m.visit_occurrence_id IN {visit_occurrence_ids}\n",
        "    AND m.provider_id IS NULL\n",
        "    AND (c.concept_id = 3010494 or c.concept_id = 3037253 or c.concept_id = 3037253 or c.concept_id = 3013308)\n",
        "    GROUP BY m.person_id, m.measurement_concept_id, c.concept_name, measurement_datetime, value_as_number, m.visit_occurrence_id\n",
        "    '''"
      ],
      "metadata": {
        "id": "MER0rdReIUo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjwI4xS2czch"
      },
      "outputs": [],
      "source": [
        "sepsis_fluid_train = pd.read_gbq(\n",
        "    fluid_intake_query.format(visit_occurrence_ids=tuple(df_train.reset_index()[\"visit_occurrence_id\"].tolist())),\n",
        "    configuration=config_gbq,\n",
        "    use_bqstorage_api=True)\n",
        "\n",
        "sepsis_fluid_train[\"value_as_number\"] = sepsis_fluid_train[\"value_as_number\"].astype(float)\n",
        "sepsis_fluid_cleaned_train = sepsis_fluid_train[sepsis_fluid_train[\"value_as_number\"] < sepsis_fluid_train[\"value_as_number\"].quantile(0.99)]\n",
        "print(sepsis_fluid_train.columns)\n",
        "sepsis_fluid_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sepsis_fluid_test = pd.read_gbq(\n",
        "    fluid_intake_query.format(visit_occurrence_ids=tuple(df_test.reset_index()[\"visit_occurrence_id\"].tolist())),\n",
        "    configuration=config_gbq,\n",
        "    use_bqstorage_api=True)\n",
        "\n",
        "sepsis_fluid_test[\"value_as_number\"] = sepsis_fluid_test[\"value_as_number\"].astype(float)\n",
        "sepsis_fluid_cleaned_test = sepsis_fluid_test[sepsis_fluid_test[\"value_as_number\"] < sepsis_fluid_test[\"value_as_number\"].quantile(0.99)]\n",
        "print(sepsis_fluid_test.columns)\n",
        "sepsis_fluid_test.head()"
      ],
      "metadata": {
        "id": "5a4P7d3SImKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNq4JsUAf5DP"
      },
      "outputs": [],
      "source": [
        "# make histogram for fluids\n",
        "fluid_ranges = {\n",
        "    0, 0, 50, 180, 530\n",
        "}\n",
        "sepsis_fluid_cleaned_train[\"value_as_number\"].hist(bins=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make histogram for fluids\n",
        "fluid_ranges = {\n",
        "    0, 0, 50, 180, 530\n",
        "}\n",
        "sepsis_fluid_cleaned_test[\"value_as_number\"].hist(bins=100)"
      ],
      "metadata": {
        "id": "0-vN_fw_PQPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_test.columns)"
      ],
      "metadata": {
        "id": "etYKv63fKCaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOGkmFURBIwn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def calculate_actions(df_sequences, df_vasopressors):\n",
        "    # Step 2: Apply the function to df2\n",
        "    tqdm.pandas()\n",
        "    gamma_sum = df_sequences.progress_apply(lambda row: compute_window_gamma(row, df_vasopressors), axis=1)\n",
        "\n",
        "    # Calculate average gamma (handle total_hours = 0 case)\n",
        "    gamma_sum[\"average_gamma\"] = gamma_sum[\"total_gamma\"] / gamma_sum['total_hours']\n",
        "    gamma_sum['average_gamma'] = gamma_sum['average_gamma'].fillna(0)  # Fill NaN for empty windows\n",
        "    return gamma_sum[\"average_gamma\"]\n",
        "\n",
        "\n",
        "def compute_window_gamma(row, df_vasopressors):\n",
        "    # Filter intervals from df1 that overlap with the current window\n",
        "    row[\"window_end\"] = row[\"measurement_datetime\"] + pd.Timedelta(hours=12)\n",
        "    row[\"window_start\"] = row[\"measurement_datetime\"]\n",
        "\n",
        "    overlapping = df_vasopressors[\n",
        "        (df_vasopressors['start_datetime'] < row['window_end']) &\\\n",
        "        (df_vasopressors['stop_datetime'] > row['window_start']) &\\\n",
        "        (df_vasopressors[\"visit_occurrence_id\"] == row[\"visit_occurrence_id\"])\n",
        "    ]\n",
        "\n",
        "    if overlapping.empty:\n",
        "        return pd.Series({'total_gamma': 0, 'total_hours': 0})\n",
        "\n",
        "    # Calculate overlap duration for normalization\n",
        "    overlapping['overlap_start'] = overlapping[\"start_datetime\"].apply(lambda x: max(x, row[\"window_start\"]))\n",
        "    overlapping['overlap_end'] = overlapping[\"stop_datetime\"].apply(lambda x: min(x, row[\"window_end\"]))\n",
        "    overlapping['overlap_duration'] = (overlapping['overlap_end'] - overlapping['overlap_start']).dt.total_seconds() / 3600  # in hours\n",
        "\n",
        "    # Compute weighted gamma\n",
        "    overlapping['weighted_gamma'] = overlapping['equiv_gamma'] * overlapping['overlap_duration']\n",
        "\n",
        "    # Aggregate results for this window\n",
        "    total_gamma = overlapping['weighted_gamma'].sum()\n",
        "    total_hours = overlapping['overlap_duration'].sum()\n",
        "    return pd.Series({'total_gamma': total_gamma, 'total_hours': total_hours})\n",
        "\n",
        "gamma_avg_train = calculate_actions(df_train[[\"visit_occurrence_id\", \"measurement_datetime\"]], sepsis_vasopressors_cleaned_train)\n",
        "gamma_avg_test = calculate_actions(df_test[[\"visit_occurrence_id\", \"measurement_datetime\"]], sepsis_vasopressors_cleaned_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm-xAdHVLPZG"
      },
      "outputs": [],
      "source": [
        "gamma_avg_train.hist(bins=100)\n",
        "print(pd.DataFrame(gamma_avg_train).quantile(0.99))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpeXpM-6M4Ys"
      },
      "outputs": [],
      "source": [
        "def add_windowed_sum_apply(df_windows, df_values, time_column, value_column, start_col, time_frame, output_col):\n",
        "    \"\"\"\n",
        "    Add a column to `df_windows` with the sum of values in `df_values` that fall within the time windows.\n",
        "\n",
        "    Parameters:\n",
        "        df_windows (pd.DataFrame): The DataFrame with time windows.\n",
        "        df_values (pd.DataFrame): The DataFrame with values and their corresponding timestamps.\n",
        "        time_column (str): The name of the time column in `df_values`.\n",
        "        value_column (str): The name of the value column in `df_values`.\n",
        "        start_col (str): The name of the start time column in `df_windows`.\n",
        "        end_col (str): The name of the end time column in `df_windows`.\n",
        "        output_col (str): The name of the column to add to `df_windows` for the sum.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The `df_windows` DataFrame with the added column.\n",
        "    \"\"\"\n",
        "    # Ensure the time columns are datetime\n",
        "    df_windows[\"window_end\"] = df_windows[start_col] + pd.Timedelta(hours=time_frame)\n",
        "\n",
        "    # Define a function to calculate the sum for a single window\n",
        "    def calculate_sum(row):\n",
        "        start_time, end_time = row[start_col], row[\"window_end\"]\n",
        "        return df_values[\n",
        "            (df_values[time_column] >= start_time) &\\\n",
        "            (df_values[time_column] <= end_time) &\\\n",
        "            (df_values[\"visit_occurrence_id\"] == row[\"visit_occurrence_id\"])\n",
        "        ][value_column].sum()\n",
        "\n",
        "    # Apply the function row-wise and create a new column\n",
        "    df_windows[output_col] = df_windows.progress_apply(calculate_sum, axis=1)\n",
        "\n",
        "    return df_windows[output_col]\n",
        "\n",
        "#df_train = df_train.reset_index()\n",
        "sepsis_fluid_cleaned_train[\"measurement_datetime\"] = pd.to_datetime(sepsis_fluid_cleaned_train[\"measurement_datetime\"])\n",
        "sepsis_fluid_cleaned_train[\"value_as_number\"] = sepsis_fluid_cleaned_train[\"value_as_number\"].astype(float)\n",
        "\n",
        "df_train[\"fluid_intake\"] = add_windowed_sum_apply(\n",
        "    df_train,\n",
        "    sepsis_fluid_cleaned_train,\n",
        "    \"measurement_datetime\",\n",
        "    \"value_as_number\",\n",
        "    \"measurement_datetime\",\n",
        "    12,\n",
        "    \"fluid_intake_sum\"\n",
        ")\n",
        "\n",
        "#df_train = df_train.reset_index()\n",
        "sepsis_fluid_cleaned_test[\"measurement_datetime\"] = pd.to_datetime(sepsis_fluid_cleaned_test[\"measurement_datetime\"])\n",
        "sepsis_fluid_cleaned_test[\"value_as_number\"] = sepsis_fluid_cleaned_test[\"value_as_number\"].astype(float)\n",
        "\n",
        "df_test[\"fluid_intake\"] = add_windowed_sum_apply(\n",
        "    df_test,\n",
        "    sepsis_fluid_cleaned_test,\n",
        "    \"measurement_datetime\",\n",
        "    \"value_as_number\",\n",
        "    \"measurement_datetime\",\n",
        "    12,\n",
        "    \"fluid_intake_sum\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rKUDWcrODXO"
      },
      "outputs": [],
      "source": [
        "df_train[\"fluid_intake_sum\"].hist(bins=100)\n",
        "print(df_train[\"fluid_intake_sum\"].mean())\n",
        "print(df_train[\"fluid_intake_sum\"].max())\n",
        "print(df_train[\"fluid_intake_sum\"].quantile(0.95))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[\"fluid_intake_sum\"].hist(bins=100)\n",
        "print(df_test[\"fluid_intake_sum\"].mean())\n",
        "print(df_test[\"fluid_intake_sum\"].max())\n",
        "print(df_test[\"fluid_intake_sum\"].quantile(0.95))"
      ],
      "metadata": {
        "id": "5BII8OqqNMpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXg72Uaf0NGd"
      },
      "source": [
        "# Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GexLWUWaRBk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def calculate_states(fluids, vaosprssors):\n",
        "    \"\"\"\n",
        "    Categorize two NumPy arrays into discrete states and compute unique scores.\n",
        "\n",
        "    Parameters:\n",
        "        array1 (numpy.ndarray): The first array.\n",
        "        array2 (numpy.ndarray): The second array.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A NumPy array of scores representing unique states (0 to 8).\n",
        "    \"\"\"\n",
        "    # Validate input\n",
        "    if len(fluids) != len(vaosprssors):\n",
        "        raise ValueError(\"Both arrays must have the same length.\")\n",
        "    median_fluids = np.median(fluids[fluids != 0])  # median for non 0 values\n",
        "    median_vaso = np.median(vaosprssors[vaosprssors != 0])  # median for non 0 values\n",
        "    # Define categories for array1\n",
        "    cat_fluids = np.zeros_like(fluids, dtype=int)\n",
        "    cat_fluids[fluids == 0] = 0\n",
        "    cat_fluids[(fluids > 0) & (fluids <= median_fluids)] = 1\n",
        "    cat_fluids[(fluids > median_fluids)] = 2\n",
        "\n",
        "    # Define categories for array2\n",
        "    cat_vaso = np.zeros_like(vaosprssors, dtype=int)\n",
        "    cat_vaso[vaosprssors == 0] = 0\n",
        "    cat_vaso[(vaosprssors > 0) & (vaosprssors <= median_vaso)] = 1\n",
        "    cat_vaso[(vaosprssors > median_vaso)] = 2\n",
        "    #print(cat2)\n",
        "    print(median_fluids)\n",
        "    print(median_vaso)\n",
        "\n",
        "    # Compute unique scores based on the combination of categories\n",
        "    # Scores  arecomputed as: cat_fluids * 3 + cat_vaso\n",
        "    scores = cat_fluids * 3 + cat_vaso\n",
        "    print(\"Fluids\", np.unique(cat_fluids, return_counts=True))\n",
        "    print(\"VPs\", np.unique(cat_vaso, return_counts=True))\n",
        "    state_mapping = {\n",
        "    \"vasopressor\": [\"0\", f\"(0-{median_fluids:.2f}]\", f\"> {median_fluids:.2f}\"],\n",
        "    \"fluid\": [\"0\", f\"(0-{median_vaso:.3f}]\", f\"> {median_vaso:.3f}\"]\n",
        "    }\n",
        "    threshold_mapping = {\n",
        "        \"vasopressor\": median_vaso,\n",
        "        \"fluid\": median_fluids\n",
        "    }\n",
        "\n",
        "    return scores, state_mapping, threshold_mapping\n",
        "\n",
        "\n",
        "def infer_states(fluids, vaosprssors, threshold_mapping):\n",
        "    \"\"\"\n",
        "    Categorize two NumPy arrays into discrete states and compute unique scores.\n",
        "\n",
        "    Parameters:\n",
        "        array1 (numpy.ndarray): The first array.\n",
        "        array2 (numpy.ndarray): The second array.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A NumPy array of scores representing unique states (0 to 8).\n",
        "    \"\"\"\n",
        "    # Validate input\n",
        "    if len(fluids) != len(vaosprssors):\n",
        "        raise ValueError(\"Both arrays must have the same length.\")\n",
        "    median_fluids = threshold_mapping[\"fluid\"]\n",
        "    median_vaso = threshold_mapping[\"vasopressor\"]\n",
        "    # Define categories for array1\n",
        "    cat_fluids = np.zeros_like(fluids, dtype=int)\n",
        "    cat_fluids[fluids == 0] = 0\n",
        "    cat_fluids[(fluids > 0) & (fluids <= median_fluids)] = 1\n",
        "    cat_fluids[(fluids > median_fluids)] = 2\n",
        "\n",
        "    # Define categories for array2\n",
        "    cat_vaso = np.zeros_like(vaosprssors, dtype=int)\n",
        "    cat_vaso[vaosprssors == 0] = 0\n",
        "    cat_vaso[(vaosprssors > 0) & (vaosprssors <= median_vaso)] = 1\n",
        "    cat_vaso[(vaosprssors > median_vaso)] = 2\n",
        "    #print(cat2)\n",
        "    print(median_fluids)\n",
        "    print(median_vaso)\n",
        "\n",
        "    # Compute unique scores based on the combination of categories\n",
        "    # Scores  arecomputed as: cat_fluids * 3 + cat_vaso\n",
        "    scores = cat_fluids * 3 + cat_vaso\n",
        "    print(\"Fluids\", np.unique(cat_fluids, return_counts=True))\n",
        "    print(\"VPs\", np.unique(cat_vaso, return_counts=True))\n",
        "\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_train[\"action\"], state_mapping, threshold_mapping = calculate_states(df_train[\"fluid_intake_sum\"].values, gamma_avg_train.values)\n",
        "df_test[\"action\"] = infer_states(df_test[\"fluid_intake_sum\"].values, gamma_avg_test.values, threshold_mapping)\n",
        "print(df_train[\"action\"].unique())\n",
        "print(df_test[\"action\"].unique())\n",
        "print(state_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mk7u2Jv2vD6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_heatmap(scores):\n",
        "    \"\"\"\n",
        "    Plot a 3x3 heatmap where each cell is colored by the count of scores (0-8).\n",
        "\n",
        "    Parameters:\n",
        "        scores (numpy.ndarray): Array of scores in the range [0, 8].\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Ensure scores are within the valid range [0, 8]\n",
        "    if not np.all((0 <= scores) & (scores <= 8)):\n",
        "        raise ValueError(\"Scores must be in the range [0, 8].\")\n",
        "\n",
        "    # Count occurrences for each score\n",
        "    counts = np.bincount(scores, minlength=9)\n",
        "\n",
        "    # Reshape counts into a 3x3 matrix\n",
        "    heatmap_matrix = counts.reshape(3, 3)\n",
        "\n",
        "    # Create the heatmap using Seaborn\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    ax = sns.heatmap(\n",
        "        heatmap_matrix,\n",
        "        annot=True,                # Annotate with count values\n",
        "        fmt=\"d\",                   # Integer format for annotations\n",
        "        cmap=\"viridis\",            # Color map\n",
        "        cbar_kws={\"label\": \"Count\"},  # Colorbar label\n",
        "        linewidths=0.5,            # Add gridlines for clarity\n",
        "        linecolor=\"white\",         # Gridline color\n",
        "        square=True                # Ensure square cells\n",
        "    )\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_title(\"Heatmap of State Counts (0-8)\", fontsize=16, fontweight=\"bold\", pad=15)\n",
        "    ax.set_xlabel(\"Vasopressor Dose Discrete\", fontsize=14, labelpad=10)\n",
        "    ax.set_ylabel(\"Fluid Intake Discrete\", fontsize=14, labelpad=10)\n",
        "    ax.set_xticks([0.5, 1.5, 2.5], [\"0\", \"1\", \"2\"])\n",
        "    ax.set_yticks([0.5, 1.5, 2.5], [\"0\", \"1\", \"2\"])\n",
        "\n",
        "    # Adjust plot layout\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "print(df_train[\"action\"].unique())\n",
        "plot_heatmap(df_train[\"action\"].values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spvPzFxDwLmQ"
      },
      "source": [
        "# Outcome Data - In hospital Mortality"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sepsis_death_query = '''\n",
        "    SELECT\n",
        "        vo.person_id,\n",
        "        vo.visit_occurrence_id,\n",
        "        d.death_datetime,\n",
        "        vo.visit_start_datetime,\n",
        "        vo.visit_end_datetime,\n",
        "        ABS(TIMESTAMP_DIFF(vo.visit_start_datetime, d.death_datetime, DAY)) AS in_hospital_survived\n",
        "    from visit_occurrence AS vo\n",
        "    LEFT JOIN death AS d\n",
        "        ON vo.person_id = d.person_id\n",
        "    WHERE vo.visit_occurrence_id IN {visit_occurrence_ids}\n",
        "    GROUP BY vo.person_id, vo.visit_occurrence_id, d.death_datetime, vo.visit_start_datetime, vo.visit_end_datetime\n",
        "    '''"
      ],
      "metadata": {
        "id": "wud1KZnqLbYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvs4GMP-wPk_"
      },
      "outputs": [],
      "source": [
        "#df_train = df_train.reset_index()\n",
        "sepsis_death_train = pd.read_gbq(\n",
        "    sepsis_death_query.format(visit_occurrence_ids=tuple(df_train[\"visit_occurrence_id\"].tolist())),\n",
        "    configuration=config_gbq,\n",
        "    use_bqstorage_api=True\n",
        ")\n",
        "# Ensure the columns are datetime objects\n",
        "sepsis_death_train['death_datetime'] = pd.to_datetime(sepsis_death_train['death_datetime'])\n",
        "sepsis_death_train['visit_end_datetime'] = pd.to_datetime(sepsis_death_train['visit_end_datetime'])\n",
        "sepsis_death_train['visit_start_datetime'] = pd.to_datetime(sepsis_death_train['visit_start_datetime'])\n",
        "\n",
        "# Set 'in_hospital_survived' to NaN where death_datetime is after visit_end_datetime\n",
        "sepsis_death_train.loc[:,'died_in_hospital'] = 0\n",
        "sepsis_death_train.loc[\n",
        "    (sepsis_death_train['death_datetime'] <= sepsis_death_train['visit_end_datetime']) &\\\n",
        "    (sepsis_death_train['death_datetime'] >= sepsis_death_train['visit_start_datetime']),\n",
        "    'died_in_hospital'\n",
        "] = 1\n",
        "sepsis_death_train['died_in_hospital'] = sepsis_death_train['died_in_hospital'].fillna(0).astype(int)\n",
        "\n",
        "sepsis_death_filtered_train = sepsis_death_train.merge(\n",
        "    df_train[[\"visit_occurrence_id\"]],\n",
        "    on=\"visit_occurrence_id\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "sepsis_death_filtered_train = sepsis_death_filtered_train.groupby(by=['visit_occurrence_id']).first().reset_index()\n",
        "\n",
        "print((sepsis_death_filtered_train[\"died_in_hospital\"]==1).sum(), \"persons died of sepsis in the hospital\")\n",
        "print((sepsis_death_filtered_train[\"died_in_hospital\"]==0).sum(), \"persons survived ICU\")\n",
        "\n",
        "sepsis_death_filtered_train[\"died_in_hospital\"].hist(bins=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df_train = df_train.reset_index()\n",
        "sepsis_death_test = pd.read_gbq(\n",
        "    sepsis_death_query.format(visit_occurrence_ids=tuple(df_test[\"visit_occurrence_id\"].tolist())),\n",
        "    configuration=config_gbq,\n",
        "    use_bqstorage_api=True\n",
        ")\n",
        "# Ensure the columns are datetime objects\n",
        "sepsis_death_test['death_datetime'] = pd.to_datetime(sepsis_death_test['death_datetime'])\n",
        "sepsis_death_test['visit_end_datetime'] = pd.to_datetime(sepsis_death_test['visit_end_datetime'])\n",
        "sepsis_death_test['visit_start_datetime'] = pd.to_datetime(sepsis_death_test['visit_start_datetime'])\n",
        "\n",
        "# Set 'in_hospital_survived' to NaN where death_datetime is after visit_end_datetime\n",
        "sepsis_death_test.loc[:,'died_in_hospital'] = 0\n",
        "sepsis_death_test.loc[\n",
        "    (sepsis_death_test['death_datetime'] <= sepsis_death_test['visit_end_datetime']) &\\\n",
        "    (sepsis_death_test['death_datetime'] >= sepsis_death_test['visit_start_datetime']),\n",
        "    'died_in_hospital'\n",
        "] = 1\n",
        "sepsis_death_test['died_in_hospital'] = sepsis_death_test['died_in_hospital'].fillna(0).astype(int)\n",
        "\n",
        "sepsis_death_filtered_test = sepsis_death_test.merge(\n",
        "    df_test[[\"visit_occurrence_id\"]],\n",
        "    on=\"visit_occurrence_id\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "sepsis_death_filtered_test = sepsis_death_filtered_test.groupby(by=['visit_occurrence_id']).first().reset_index()\n",
        "\n",
        "print((sepsis_death_filtered_test[\"died_in_hospital\"]==1).sum(), \"persons died of sepsis in the hospital\")\n",
        "print((sepsis_death_filtered_test[\"died_in_hospital\"]==0).sum(), \"persons survived ICU\")\n",
        "\n",
        "sepsis_death_filtered_test[\"died_in_hospital\"].hist(bins=2)"
      ],
      "metadata": {
        "id": "cxh2slbELwjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4brdgSOGl44I"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the percentage distribution\n",
        "counts = sepsis_death_filtered_train[\"died_in_hospital\"].value_counts(normalize=True) * 100\n",
        "\n",
        "# Create a DataFrame for seaborn\n",
        "plot_data = counts.reset_index()\n",
        "plot_data.columns = [\"Died in Hospital\", \"Percentage\"]\n",
        "\n",
        "# Create the Seaborn barplot\n",
        "sns.set_theme(style=\"whitegrid\")  # Set Seaborn theme\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "palette = [\"#76c893\", \"#f94144\"]  # Custom colors for 'No' and 'Yes'\n",
        "\n",
        "sns.barplot(\n",
        "    x=\"Died in Hospital\", y=\"Percentage\", data=plot_data,\n",
        "    palette=palette, ax=ax, saturation=0.85, edgecolor=\"black\"\n",
        ")\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_title(\"Percentage of Patients Who Died in the Hospital\", fontsize=16, fontweight=\"bold\", pad=15)\n",
        "ax.set_xlabel(\"Died in Hospital (0 = No, 1 = Yes)\", fontsize=14, fontweight=\"bold\", labelpad=10)\n",
        "ax.set_ylabel(\"Percentage (%)\", fontsize=14, fontweight=\"bold\", labelpad=10)\n",
        "\n",
        "# Annotate the bars with percentages\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f\"{p.get_height():.1f}%\",\n",
        "                (p.get_x() + p.get_width() / 2, p.get_height()),\n",
        "                ha=\"center\", va=\"bottom\", fontsize=14, fontweight=\"bold\", color=\"black\")\n",
        "\n",
        "# Remove unnecessary spines\n",
        "sns.despine(left=True, bottom=False)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nx7-0PkgALh"
      },
      "outputs": [],
      "source": [
        "# Making the binary in-hospital death\n",
        "print(len(df_train))\n",
        "df_train = df_train.merge(\n",
        "    sepsis_death_filtered_train[[\"visit_occurrence_id\", \"died_in_hospital\"]],\n",
        "    on=\"visit_occurrence_id\",\n",
        "    how=\"left\"\n",
        ")\n",
        "print(len(df_train))\n",
        "print(df_train[\"died_in_hospital\"].unique())\n",
        "df_train[\"died_in_hospital\"] = df_train[\"died_in_hospital\"].fillna(0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the binary in-hospital death\n",
        "print(len(df_test))\n",
        "df_test = df_test.merge(\n",
        "    sepsis_death_filtered_test[[\"visit_occurrence_id\", \"died_in_hospital\"]],\n",
        "    on=\"visit_occurrence_id\",\n",
        "    how=\"left\"\n",
        ")\n",
        "print(len(df_test))\n",
        "print(df_test[\"died_in_hospital\"].unique())\n",
        "df_test[\"died_in_hospital\"] = df_test[\"died_in_hospital\"].fillna(0).astype(int)"
      ],
      "metadata": {
        "id": "HAMZrPHBMF9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCJ9zegH1fC9"
      },
      "outputs": [],
      "source": [
        "df_train[\"reward\"] = 0\n",
        "df_train = df_train.sort_values(by=[\"visit_occurrence_id\", \"measurement_datetime\"])\n",
        "# Add sequence_id within each visit_occurrence_id\n",
        "df_train[\"seq_id\"] = df_train.groupby(\"visit_occurrence_id\").cumcount() + 1\n",
        "# Find the row index for the maximum sequence_id for each visit_occurrence_id\n",
        "max_sequence_indices = df_train.groupby(\"visit_occurrence_id\")[\"seq_id\"].idxmax()\n",
        "print(df_train[[\"visit_occurrence_id\", \"seq_id\", \"died_in_hospital\"]])\n",
        "\n",
        "# Set reward to 100 or -100 based on died_in_hospital flag\n",
        "df_train.loc[max_sequence_indices, \"reward\"] = df_train.loc[max_sequence_indices, \"died_in_hospital\"].apply(\n",
        "    lambda x: 100 if x == 0.0 else -100\n",
        ")\n",
        "\n",
        "df_train[\"reward\"].hist(bins=3)\n",
        "print(df_train[\"reward\"].unique())\n",
        "print(df_train[\"reward\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[\"reward\"] = 0\n",
        "df_test = df_test.sort_values(by=[\"visit_occurrence_id\", \"measurement_datetime\"])\n",
        "# Add sequence_id within each visit_occurrence_id\n",
        "df_test[\"seq_id\"] = df_test.groupby(\"visit_occurrence_id\").cumcount() + 1\n",
        "# Find the row index for the maximum sequence_id for each visit_occurrence_id\n",
        "max_sequence_indices = df_test.groupby(\"visit_occurrence_id\")[\"seq_id\"].idxmax()\n",
        "print(df_test[[\"visit_occurrence_id\", \"seq_id\", \"died_in_hospital\"]])\n",
        "\n",
        "# Set reward to 100 or -100 based on died_in_hospital flag\n",
        "df_test.loc[max_sequence_indices, \"reward\"] = df_test.loc[max_sequence_indices, \"died_in_hospital\"].apply(\n",
        "    lambda x: 100 if x == 0.0 else -100\n",
        ")\n",
        "\n",
        "df_test[\"reward\"].hist(bins=3)\n",
        "print(df_test[\"reward\"].unique())\n",
        "print(df_test[\"reward\"].value_counts())"
      ],
      "metadata": {
        "id": "1vjVgyRmMVfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otD9LODs48c1"
      },
      "source": [
        "# Obtaining States"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-twTwmE4p-g"
      },
      "outputs": [],
      "source": [
        "# Clustering\n",
        "from src.clustering import cluster_kmpp\n",
        "# chose numeric columns\n",
        "print(df_train.columns)\n",
        "df_train_clust = df_train[['Alanine aminotransferase', 'Albumin', 'Aspartate aminotransferase',\n",
        "       'Base excess', 'Bilirubin.total', 'Calcium',\n",
        "       'Calcium.ionized', 'Chloride', 'Creatinine', 'Diastolic blood pressure',\n",
        "       'FiO2', 'Glucose', 'Heart Rate', 'Hemoglobin', 'INR', 'Lactate',\n",
        "       'Leukocytes', 'Magnesium', 'Mean Blood Pressure', 'PT', 'PaOxygen',\n",
        "       'Platelets', 'Potassium', 'Respiratory rate', 'Sodium', 'SpO2',\n",
        "       'Systolic Blood Pressure', 'Temperature', 'Urea', 'Urine', 'Weight',\n",
        "       'aPTT', 'pH Blood', 'paCO2', 'sofa_cns_score', 'ventilatory_support',\n",
        "       'pao2_fio2_ratio', 'pao2', 'Gender', 'age_at_visit']]\n",
        "df_test_clust = df_test[['Alanine aminotransferase', 'Albumin', 'Aspartate aminotransferase',\n",
        "       'Base excess', 'Bilirubin.total', 'Calcium',\n",
        "       'Calcium.ionized', 'Chloride', 'Creatinine', 'Diastolic blood pressure',\n",
        "       'FiO2', 'Glucose', 'Heart Rate', 'Hemoglobin', 'INR', 'Lactate',\n",
        "       'Leukocytes', 'Magnesium', 'Mean Blood Pressure', 'PT', 'PaOxygen',\n",
        "       'Platelets', 'Potassium', 'Respiratory rate', 'Sodium', 'SpO2',\n",
        "       'Systolic Blood Pressure', 'Temperature', 'Urea', 'Urine', 'Weight',\n",
        "       'aPTT', 'pH Blood', 'paCO2', 'sofa_cns_score', 'ventilatory_support',\n",
        "       'pao2_fio2_ratio', 'pao2', 'Gender', 'age_at_visit']]\n",
        "\n",
        "kmeans, cluster_centers = cluster_kmpp(df_train_clust, k=400)\n",
        "df_train[\"state\"] = kmeans.predict(df_train_clust)\n",
        "df_test[\"state\"] = kmeans.predict(df_test_clust)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMdH2IPtke_8"
      },
      "outputs": [],
      "source": [
        "df_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_clust.describe()"
      ],
      "metadata": {
        "id": "iCG-_dqv2kf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWEkXM2gboLK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Sample data (replace with your clustered data)\n",
        "data = df_train_clust.copy().values  # Replace with your data\n",
        "cluster_labels = df_train[\"state\"].values  # Replace with your cluster labels\n",
        "\n",
        "# Calculate cluster centers and sizes\n",
        "unique_clusters = np.unique(cluster_labels)\n",
        "cluster_centers = np.array([data[cluster_labels == cluster].mean(axis=0) for cluster in unique_clusters])\n",
        "cluster_sizes = np.array([np.sum(cluster_labels == cluster) for cluster in unique_clusters])  # Cluster population\n",
        "\n",
        "# Apply PCA to reduce cluster centers to 3D\n",
        "pca = PCA(n_components=3)\n",
        "pca_results = pca.fit_transform(cluster_centers)\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "pca_df = pd.DataFrame(pca_results, columns=[\"PCA-1\", \"PCA-2\", \"PCA-3\"])\n",
        "pca_df[\"Cluster\"] = unique_clusters\n",
        "pca_df[\"Size\"] = cluster_sizes\n",
        "\n",
        "# Generate a color palette for distinct colors\n",
        "color_palette = plt.cm.get_cmap(\"tab10\", len(unique_clusters))\n",
        "colors = [color_palette(cluster) for cluster in unique_clusters]\n",
        "\n",
        "# Normalize cluster sizes for plotting\n",
        "marker_sizes = (pca_df[\"Size\"] / pca_df[\"Size\"].max()) * 300 + 100  # Scale marker sizes\n",
        "\n",
        "# 3D Plot using Matplotlib\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Scatter plot for cluster centers\n",
        "scatter = ax.scatter(\n",
        "    pca_df[\"PCA-1\"], pca_df[\"PCA-2\"], pca_df[\"PCA-3\"],\n",
        "    c=colors, s=marker_sizes, alpha=0.9, edgecolor=\"black\"\n",
        ")\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_title(\"3D PCA Visualization of Cluster Centers (Size Scaled by Population)\", fontsize=16, fontweight=\"bold\")\n",
        "ax.set_xlabel(\"PCA-1\", fontsize=12)\n",
        "ax.set_ylabel(\"PCA-2\", fontsize=12)\n",
        "ax.set_zlabel(\"PCA-3\", fontsize=12)\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0_P9i7Q71_S"
      },
      "outputs": [],
      "source": [
        "del data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Hp6OPdOky79"
      },
      "outputs": [],
      "source": [
        "df_train[[\"state\", \"action\", \"reward\", \"seq_id\"]].head(50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[[\"state\", \"action\", \"reward\", \"seq_id\"]].head(50)"
      ],
      "metadata": {
        "id": "qtJY59rWM8CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5BXh58Hq7e0"
      },
      "source": [
        "# Transform df for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y-Y68MTq6-f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "df_test_rl = df_test[['state','action','reward','seq_id','visit_occurrence_id']]\n",
        "\n",
        "# Initialize an empty list to store trajectory data\n",
        "trajectory_data = []\n",
        "\n",
        "# Group by 'id' to handle each trajectory independently\n",
        "for _, group in df_test_rl.groupby('visit_occurrence_id'):\n",
        "    group = group.sort_values('seq_id')  # Ensure the sequence is sorted\n",
        "    states = group['state'].tolist()\n",
        "    actions = group['action'].tolist()\n",
        "    rewards = group['reward'].tolist()\n",
        "\n",
        "    # Create next_state by shifting 'state' column\n",
        "    next_states = states[1:] + [-1]  # Last next_state is None\n",
        "\n",
        "    # Create done flag (1 for the last step, 0 otherwise)\n",
        "    done_flags = [0] * (len(states) - 1) + [1]\n",
        "\n",
        "    # Build the trajectory data\n",
        "    for i in range(len(states)):\n",
        "        trajectory_data.append({\n",
        "            'state': states[i],\n",
        "            'action': actions[i],\n",
        "            'reward': rewards[i],\n",
        "            'next_state': next_states[i],\n",
        "            'done': done_flags[i]\n",
        "        })\n",
        "\n",
        "# Convert trajectory data to a DataFrame\n",
        "trajectory_data = pd.DataFrame(trajectory_data)\n",
        "\n",
        "# Display the result\n",
        "print(trajectory_data)\n",
        "print(trajectory_data.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_space_size, action_space_size, learning_rate=0.1, discount_factor=0.99):\n",
        "        \"\"\"\n",
        "        Initialize the Q-learning agent.\n",
        "\n",
        "        Parameters:\n",
        "        - state_space_size: Number of states (discretized features).\n",
        "        - action_space_size: Number of discrete actions.\n",
        "        - learning_rate: Step size for updating Q-values.\n",
        "        - discount_factor: Discount factor for future rewards.\n",
        "        \"\"\"\n",
        "        self.state_space_size = state_space_size\n",
        "        self.action_space_size = action_space_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.q_table = np.zeros((state_space_size, action_space_size), dtype=np.float64)  # Initialize Q-table\n",
        "\n",
        "    def update_batch(self, states, actions, rewards, next_states):\n",
        "        \"\"\"\n",
        "        Update the Q-table using a batch of transitions, considering terminal states represented as -1.\n",
        "\n",
        "        Parameters:\n",
        "        - states: Array of current states.\n",
        "        - actions: Array of actions taken.\n",
        "        - rewards: Array of observed rewards.\n",
        "        - next_states: Array of resulting next states, with -1 for terminal states.\n",
        "        \"\"\"\n",
        "        td_targets = rewards.copy()\n",
        "\n",
        "        # Mask for non-terminal states\n",
        "        non_terminal_mask = next_states != -1\n",
        "\n",
        "        # For non-terminal states, include the discounted future rewards\n",
        "        if np.any(non_terminal_mask):\n",
        "            next_states_non_terminal = next_states[non_terminal_mask].astype(int)  # Convert valid states to int\n",
        "            best_next_actions = np.argmax(self.q_table[next_states_non_terminal], axis=1)\n",
        "            td_targets[non_terminal_mask] += (\n",
        "                self.discount_factor * self.q_table[next_states_non_terminal, best_next_actions]\n",
        "            )\n",
        "\n",
        "        # Ensure td_targets dtype matches q_table dtype\n",
        "        td_targets = td_targets.astype(self.q_table.dtype)\n",
        "\n",
        "        # Update Q-values\n",
        "        for i in range(len(states)):\n",
        "            td_error = td_targets[i] - self.q_table[states[i], actions[i]]\n",
        "            self.q_table[states[i], actions[i]] += self.learning_rate * td_error\n",
        "\n",
        "# Define state and action space sizes\n",
        "state_space_size = trajectory_data['state'].max() + 1\n",
        "action_space_size = trajectory_data['action'].max() + 1\n",
        "\n",
        "# Initialize Q-learning agent\n",
        "agent = QLearningAgent(state_space_size, action_space_size)\n",
        "\n",
        "# Training with batch processing\n",
        "batch_size = 2\n",
        "for i in range(0, len(trajectory_data), batch_size):\n",
        "    # Extract batch\n",
        "    batch = trajectory_data.iloc[i:i + batch_size]\n",
        "    states = batch['state'].values\n",
        "    actions = batch['action'].values\n",
        "    rewards = batch['reward'].astype(float).values\n",
        "    next_states = batch['next_state'].values\n",
        "\n",
        "    # Update Q-table using the batch\n",
        "    agent.update_batch(states, actions, rewards, next_states)\n",
        "\n",
        "# Print the trained Q-table\n",
        "print(\"Trained Q-Table:\")\n",
        "print(agent.q_table)\n"
      ],
      "metadata": {
        "id": "UIhlDZAi751q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as mcolors\n",
        "# Append optimal actions from Q-table to trajectory data\n",
        "output_data = trajectory_data.copy()\n",
        "output_data['optimal_action'] = trajectory_data['state'].apply(lambda s: np.argmax(agent.q_table[s]))\n",
        "\n",
        "\n",
        "# 3D Bar Plot for Action Frequency Based on Actions Column\n",
        "action_counts = np.bincount(output_data['action'], minlength=9)\n",
        "action_counts = action_counts / sum(action_counts)\n",
        "optimal_action_counts = np.bincount(output_data['optimal_action'], minlength=9)\n",
        "optimal_action_counts = optimal_action_counts / sum(optimal_action_counts)\n",
        "print(action_counts)\n",
        "z_bar_height = max(optimal_action_counts.max(), action_counts.max())\n",
        "\n",
        "# fuild * 3 + vaso\n",
        "# Prepare a 3x3 grid for actions\n",
        "\n",
        "action_grid_x, action_grid_y = np.meshgrid(range(3), range(3))\n",
        "action_grid_z = action_counts.reshape(3, 3)\n",
        "optimal_action_grid_z = optimal_action_counts.reshape(3, 3)\n",
        "print(action_grid_z)\n",
        "\n",
        "# Create the 3D bar plot\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "ax2 = fig.add_subplot(122, projection='3d')\n",
        "\n",
        "xpos = action_grid_x.ravel()\n",
        "ypos = action_grid_y.ravel()\n",
        "zpos = np.zeros_like(xpos)\n",
        "\n",
        "\n",
        "\n",
        "for ax, bar_heights, title in [\n",
        "    (ax1, action_grid_z, 'Action Frequency Based on\\n Clinicians Actions'),\n",
        "    (ax2, optimal_action_grid_z, 'Action Frequency Based on\\n Q-Learning Actions')\n",
        "]:\n",
        "    dx = dy = 0.8  # Width of the bars\n",
        "    dz = bar_heights.flatten()  # Heights of the bars\n",
        "\n",
        "    # Normalize the bar heights (dz) for the colormap\n",
        "    norm = mcolors.Normalize(vmin=dz.min(), vmax=dz.max())\n",
        "    cmap = cm.viridis  # Choose a colormap\n",
        "\n",
        "    # Map the bar heights (dz) to colors\n",
        "    colors = cmap(norm(dz))\n",
        "\n",
        "    ax.bar3d(xpos, ypos, zpos, dx, dy, dz, shade=True,  color=colors, edgecolor='white')\n",
        "    #ax.view_init(elev=30, roll=0)\n",
        "    ax.set_box_aspect(aspect=(1,1,1), zoom=0.9)\n",
        "    ax.set_xlabel('Vasopressor dose        ', labelpad=10)\n",
        "    ax.set_ylabel('       Intravenous\\n          fluids dose', labelpad=30)\n",
        "    ax.set_xticks([0.5, 1.5, 2.5], state_mapping[\"vasopressor\"])\n",
        "    ax.tick_params(axis='y', pad=10)  # Add padding for x-axis\n",
        "    ax.set_yticks([0.5, 1.5, 2.5], state_mapping[\"fluid\"])\n",
        "\n",
        "    ax.set_zlabel('Action %', labelpad=20)\n",
        "    ax.tick_params(axis='z', pad=10)  # Add padding for x-axis\n",
        "    ax.set_zlim(0, z_bar_height)  # Replace max_height with your desired maximum height\n",
        "\n",
        "    ax.set_title(title)\n",
        "\n",
        "    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])  # Required for colorbar\n",
        "    cbar = plt.colorbar(sm, ax=ax, shrink=0.4, pad=0.2)  # Adjust size and position\n",
        "    cbar.set_label('Bar Height', labelpad=10)\n",
        "\n",
        "\n",
        "ax.dist = 20\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2LiLDcu9dxZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "kP6TL4_eC2i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from src.dkm import DKNLoss, Autoencoder\n",
        "from src.dataset import AmsICUSepticShock\n",
        "\n",
        "\n",
        "import optuna\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_and_evaluate_models_from_optuna(\n",
        "    study_name,\n",
        "    storage_url,\n",
        "    input_dim,\n",
        "    model_class,\n",
        "    validation_data,\n",
        "    criterion,\n",
        "    model_weight_prefix=\"./MLRFH/models/\",\n",
        "    batch_size=64\n",
        "):\n",
        "    \"\"\"\n",
        "    Load and evaluate all models saved from an Optuna study and return the best model.\n",
        "\n",
        "    Args:\n",
        "        study_name (str): Name of the Optuna study.\n",
        "        storage_url (str): URL to the Optuna storage (e.g., \"sqlite:///optuna_study.db\").\n",
        "        input_dim (int): Input dimension of the Autoencoder.\n",
        "        model_class (class): The Autoencoder class to initialize models.\n",
        "        validation_data (torch.utils.data.Dataset): Validation dataset.\n",
        "        criterion (torch.nn.Module): Loss function for evaluation.\n",
        "        model_weight_prefix (str): Path prefix for the saved model weights.\n",
        "        batch_size (int): Batch size for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (best_model, best_params, best_loss)\n",
        "    \"\"\"\n",
        "    # Load the Optuna study\n",
        "    study = optuna.load_study(study_name=study_name, storage=storage_url)\n",
        "    print(study)\n",
        "\n",
        "    # DataLoader for validation data\n",
        "    validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device {device} for evaluation.\")\n",
        "\n",
        "    best_model = None\n",
        "    best_params = None\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    for trial in study.trials:\n",
        "        # Get trial parameters\n",
        "        params = trial.params\n",
        "        latent_dim = params[\"latent_dim\"]\n",
        "        layer_sizes = params[\"layer_sizes\"]\n",
        "        trial_number = trial.number\n",
        "\n",
        "        print(f\"Evaluating model from trial {trial_number} with parameters: {params}\")\n",
        "\n",
        "        # Initialize the model\n",
        "        model = model_class(input_dim, latent_dim=latent_dim, layer_sizes=layer_sizes, k=400)\n",
        "        model = model.to(device)\n",
        "\n",
        "        # Load model weights\n",
        "        model_path = f\"{model_weight_prefix}dkm_model_{trial_number}.pth\"\n",
        "        try:\n",
        "            state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "            #print(state_dict[\"cluster_centers\"])\n",
        "            model.load_state_dict(state_dict)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Model weights not found for trial {trial_number} at {model_path}. Skipping...\")\n",
        "            continue\n",
        "        #except RuntimeError:\n",
        "        #    print(f\"Error loading model weights for trial {trial_number}. Skipping...\")\n",
        "        #    continue\n",
        "\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "        # Evaluate the model on validation data\n",
        "        total_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in validation_loader:\n",
        "                x = batch.to(device)\n",
        "                a_x, h_x = model(x)  # Forward pass\n",
        "\n",
        "                # Compute loss\n",
        "                loss, _, _ = criterion(x, h_x, a_x, model.cluster_centers)  # Pass cluster centers as None if not used\n",
        "                total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(validation_loader.dataset)\n",
        "        print(f\"Validation Loss for trial {trial_number}: {avg_loss}\")\n",
        "\n",
        "        # Update the best model if this one is better\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            best_model = model\n",
        "            best_params = params\n",
        "\n",
        "    print(f\"Best Model: Trial {trial_number} with Loss: {best_loss}\")\n",
        "    return best_model, best_params, best_loss\n",
        "\n",
        "\n",
        "study_name = \"deep_kmeans_autoencoder\"\n",
        "storage_url = \"sqlite:///optuna_study_final.db\"\n",
        "input_dim = len(df_val.columns)  # Adjust as per your data\n",
        "\n",
        "# Validation dataset and criterion\n",
        "validation_data = AmsICUSepticShock(df_val)\n",
        "criterion = DKNLoss()\n",
        "\n",
        "# Load and evaluate models\n",
        "best_model, best_params, best_loss = load_and_evaluate_models_from_optuna(\n",
        "    study_name=study_name,\n",
        "    storage_url=storage_url,\n",
        "    input_dim=input_dim,\n",
        "    model_class=Autoencoder,\n",
        "    validation_data=validation_data,\n",
        "    criterion=criterion\n",
        ")\n",
        "\n",
        "print(\"Best model parameters:\", best_params)\n",
        "print(\"Best validation loss:\", best_loss)\n"
      ],
      "metadata": {
        "id": "D_7nxTcK4fnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_features = ['Alanine aminotransferase', 'Albumin', 'Aspartate aminotransferase',\n",
        "       'Base excess', 'Bilirubin.total', 'Calcium',\n",
        "       'Calcium.ionized', 'Chloride', 'Creatinine', 'Diastolic blood pressure',\n",
        "       'FiO2', 'Glucose', 'Heart Rate', 'Hemoglobin', 'INR', 'Lactate',\n",
        "       'Leukocytes', 'Magnesium', 'Mean Blood Pressure', 'PT', 'PaOxygen',\n",
        "       'Platelets', 'Potassium', 'Respiratory rate', 'Sodium', 'SpO2',\n",
        "       'Systolic Blood Pressure', 'Temperature', 'Urea', 'Urine', 'Weight',\n",
        "       'aPTT', 'pH Blood', 'paCO2', 'sofa_cns_score', 'ventilatory_support',\n",
        "       'pao2_fio2_ratio', 'pao2', 'Gender', 'age_at_visit']"
      ],
      "metadata": {
        "id": "_ukFMq11--tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import optuna\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def load_model_and_assign_states_from_study(\n",
        "    study_name,\n",
        "    storage_url,\n",
        "    trial_index,\n",
        "    model_class,\n",
        "    data,\n",
        "    device=None,\n",
        "    model_weight_prefix=\"./MLRFH/models/\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Load a specific model using its trial index from an Optuna study, embed the data, and assign each datapoint to a cluster state.\n",
        "\n",
        "    Args:\n",
        "        study_name (str): Name of the Optuna study.\n",
        "        storage_url (str): URL to the Optuna storage (e.g., \"sqlite:///optuna_study.db\").\n",
        "        trial_index (int): Index of the trial to load.\n",
        "        model_class (class): The Autoencoder class to initialize models.\n",
        "        data (torch.Tensor): Dataset to embed and assign states (shape: [num_samples, input_dim]).\n",
        "        device (torch.device, optional): Device to run the model on. Defaults to CPU if not provided.\n",
        "        model_weight_prefix (str): Path prefix for the saved model weights.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (embedded_data, assigned_states)\n",
        "               - embedded_data (np.ndarray): The latent embeddings of the data.\n",
        "               - assigned_states (np.ndarray): The cluster index for each data point.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load the Optuna study\n",
        "    study = optuna.load_study(study_name=study_name, storage=storage_url)\n",
        "\n",
        "    # Retrieve the trial based on the index\n",
        "    trial = next((t for t in study.trials if t.number == trial_index), None)\n",
        "    if trial is None:\n",
        "        raise ValueError(f\"Trial with index {trial_index} not found in the study.\")\n",
        "\n",
        "    # Get trial parameters\n",
        "    params = trial.params\n",
        "    latent_dim = params[\"latent_dim\"]\n",
        "    layer_sizes = params[\"layer_sizes\"]\n",
        "\n",
        "    print(f\"Loading model for trial {trial_index} with parameters: {params}\")\n",
        "\n",
        "    # Initialize the model\n",
        "    model = model_class(\n",
        "        input_dim=data.size(1),  # Assuming data is a tensor with shape [num_samples, input_dim]\n",
        "        latent_dim=latent_dim,\n",
        "        layer_sizes=layer_sizes,\n",
        "        k=400  # You can adjust this if the value is stored elsewhere\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Load model weights\n",
        "    model_path = f\"{model_weight_prefix}dkm_model_{trial_index}.pth\"\n",
        "    try:\n",
        "        state_dict = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(state_dict)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Model weights not found for trial {trial_index} at {model_path}.\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Extract cluster centers\n",
        "    cluster_centers = model.cluster_centers.to(device)\n",
        "\n",
        "    # Embed the data\n",
        "    print(\"Embedding data...\")\n",
        "    data = data.to(device)\n",
        "    with torch.no_grad():\n",
        "        _, latent_representations = model(data)  # Assuming model returns (reconstructed, latent)\n",
        "        embedded_data = latent_representations.cpu().numpy()\n",
        "\n",
        "    # Assign each data point to the nearest cluster center\n",
        "    print(\"Assigning states...\")\n",
        "    with torch.no_grad():\n",
        "        distances = torch.cdist(latent_representations, cluster_centers, p=2)  # Pairwise distances\n",
        "        assigned_states = torch.argmin(distances, dim=1).cpu().numpy()\n",
        "\n",
        "    return embedded_data, assigned_states, model.cluster_centers.detach().numpy()\n",
        "\n",
        "\n",
        "study_name = \"deep_kmeans_autoencoder\"\n",
        "storage_url = \"sqlite:///optuna_study_final.db\"\n",
        "trial_index = 4  # Example trial index\n",
        "num_clusters = 400\n",
        "\n",
        "# Load and use the model\n",
        "embedded_data_train, assigned_states_train, cluster_centers = load_model_and_assign_states_from_study(\n",
        "    study_name=study_name,\n",
        "    storage_url=storage_url,\n",
        "    trial_index=trial_index,\n",
        "    model_class=Autoencoder,\n",
        "    data=torch.Tensor(df_train[use_features].values)\n",
        ")\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(\"Latent Embeddings:\")\n",
        "print(embedded_data_train)\n",
        "print(\"Assigned States:\")\n",
        "print(len(np.unique(assigned_states_train)))\n",
        "\n",
        "embedded_data_test, assigned_states_test, _ = load_model_and_assign_states_from_study(\n",
        "    study_name=study_name,\n",
        "    storage_url=storage_url,\n",
        "    trial_index=trial_index,\n",
        "    model_class=Autoencoder,\n",
        "    data=torch.Tensor(df_test[use_features].values)\n",
        ")\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(\"Latent Embeddings:\")\n",
        "print(embedded_data_test)\n",
        "print(\"Assigned States:\")\n",
        "print(len(np.unique(assigned_states_test)))\n"
      ],
      "metadata": {
        "id": "I-6KrjnhEL3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def assign_to_closest_center(embedded_data, cluster_centers):\n",
        "    \"\"\"\n",
        "    Assign each data point in embedded_data to the closest cluster center.\n",
        "\n",
        "    Parameters:\n",
        "        embedded_data (numpy.ndarray): Array of shape (n_samples, n_features) representing the embedded data points.\n",
        "        cluster_centers (numpy.ndarray): Array of shape (n_clusters, n_features) representing the cluster centers.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array of shape (n_samples,) containing the index of the closest cluster center for each data point.\n",
        "    \"\"\"\n",
        "    # Compute the distances between each data point and each cluster center\n",
        "    distances = np.linalg.norm(embedded_data[:, np.newaxis] - cluster_centers, axis=2)\n",
        "\n",
        "    # Find the index of the closest cluster center for each data point\n",
        "    assigned_states = np.argmin(distances, axis=1)\n",
        "\n",
        "    return assigned_states\n",
        "\n",
        "\n",
        "\n",
        "# Assign each point to the closest cluster center\n",
        "assigned_states_train = assign_to_closest_center(embedded_data_train, cluster_centers)\n",
        "assigned_states_test = assign_to_closest_center(embedded_data_test, cluster_centers)\n",
        "print(\"Assigned cluster indices:\", assigned_states_train)"
      ],
      "metadata": {
        "id": "XN2F5tNr_Z3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Sample data (replace with your clustered data)\n",
        "data = df_test_clust.copy().values  # Replace with your data\n",
        "cluster_labels = df_test[\"state\"].values  # Replace with your cluster labels\n",
        "\n",
        "# Calculate cluster centers and sizes\n",
        "unique_clusters = len(cluster_centers)\n",
        "#cluster_centers =\n",
        "cluster_sizes = np.ones(shape=cluster_centers.shape[0])  # Cluster population\n",
        "\n",
        "# Apply PCA to reduce cluster centers to 3D\n",
        "pca = PCA(n_components=3)\n",
        "pca_results = pca.fit_transform(cluster_centers)\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "pca_df = pd.DataFrame(pca_results, columns=[\"PCA-1\", \"PCA-2\", \"PCA-3\"])\n",
        "pca_df[\"Cluster\"] = unique_clusters\n",
        "pca_df[\"Size\"] = cluster_sizes\n",
        "\n",
        "# Generate a color palette for distinct colors\n",
        "color_palette = plt.cm.get_cmap(\"tab10\", unique_clusters)\n",
        "colors = [color_palette(cluster) for cluster in range(unique_clusters)]\n",
        "\n",
        "# Normalize cluster sizes for plotting\n",
        "marker_sizes = (pca_df[\"Size\"] / pca_df[\"Size\"].max()) * 300 + 100  # Scale marker sizes\n",
        "\n",
        "# 3D Plot using Matplotlib\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Scatter plot for cluster centers\n",
        "scatter = ax.scatter(\n",
        "    pca_df[\"PCA-1\"], pca_df[\"PCA-2\"], pca_df[\"PCA-3\"],\n",
        "    c=colors, s=marker_sizes, alpha=0.9, edgecolor=\"black\"\n",
        ")\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_title(\"3D PCA Visualization of Cluster Centers (Size Scaled by Population)\", fontsize=16, fontweight=\"bold\")\n",
        "ax.set_xlabel(\"PCA-1\", fontsize=12)\n",
        "ax.set_ylabel(\"PCA-2\", fontsize=12)\n",
        "ax.set_zlabel(\"PCA-3\", fontsize=12)\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wWGxMdfTQxfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "\n",
        "def compute_similarity_scores(labels1, labels2):\n",
        "    \"\"\"\n",
        "    Compute similarity scores between two clusterings.\n",
        "\n",
        "    Parameters:\n",
        "        labels1 (list or array): Cluster labels of the first clustering.\n",
        "        labels2 (list or array): Cluster labels of the second clustering.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing NMI, ARI, and V-Measure scores.\n",
        "    \"\"\"\n",
        "    scores = {\n",
        "        'NMI': normalized_mutual_info_score(labels1, labels2),\n",
        "        'ARI': adjusted_rand_score(labels1, labels2),\n",
        "    }\n",
        "    return scores\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example cluster labels\n",
        "    clustering1 = assigned_states_train\n",
        "    clustering2 = df_train[\"state\"].values\n",
        "\n",
        "    similarity_scores = compute_similarity_scores(clustering1, clustering2)\n",
        "\n",
        "    print(\"Similarity Scores:\")\n",
        "    for metric, score in similarity_scores.items():\n",
        "        print(f\"{metric}: {score:.3f}\")"
      ],
      "metadata": {
        "id": "qeww3pMiSukF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame\n",
        "df_train_emb_rl = df_test[['state','action','reward','seq_id','visit_occurrence_id']]\n",
        "df_train_emb_rl['state'] = assigned_states_test\n",
        "# Initialize an empty list to store trajectory data\n",
        "trajectory_data_emb = []\n",
        "\n",
        "# Group by 'id' to handle each trajectory independently\n",
        "for _, group in df_train_emb_rl.groupby('visit_occurrence_id'):\n",
        "    group = group.sort_values('seq_id')  # Ensure the sequence is sorted\n",
        "    states = group['state'].tolist()\n",
        "    actions = group['action'].tolist()\n",
        "    rewards = group['reward'].tolist()\n",
        "\n",
        "    # Create next_state by shifting 'state' column\n",
        "    next_states = states[1:] + [-1]  # Last next_state is None\n",
        "\n",
        "    # Create done flag (1 for the last step, 0 otherwise)\n",
        "    done_flags = [0] * (len(states) - 1) + [1]\n",
        "\n",
        "    # Build the trajectory data\n",
        "    for i in range(len(states)):\n",
        "        trajectory_data_emb.append({\n",
        "            'state': states[i],\n",
        "            'action': actions[i],\n",
        "            'reward': rewards[i],\n",
        "            'next_state': next_states[i],\n",
        "            'done': done_flags[i]\n",
        "        })\n",
        "\n",
        "# Convert trajectory data to a DataFrame\n",
        "trajectory_data_emb = pd.DataFrame(trajectory_data_emb)\n",
        "\n",
        "# Display the result\n",
        "print(trajectory_data_emb)\n",
        "print(trajectory_data_emb.dtypes)"
      ],
      "metadata": {
        "id": "XfbVZ9CaCVZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_space_size, action_space_size, learning_rate=0.1, discount_factor=0.99):\n",
        "        \"\"\"\n",
        "        Initialize the Q-learning agent.\n",
        "\n",
        "        Parameters:\n",
        "        - state_space_size: Number of states (discretized features).\n",
        "        - action_space_size: Number of discrete actions.\n",
        "        - learning_rate: Step size for updating Q-values.\n",
        "        - discount_factor: Discount factor for future rewards.\n",
        "        \"\"\"\n",
        "        self.state_space_size = state_space_size\n",
        "        self.action_space_size = action_space_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.q_table = np.zeros((state_space_size, action_space_size), dtype=np.float64)  # Initialize Q-table\n",
        "\n",
        "    def update_batch(self, states, actions, rewards, next_states):\n",
        "        \"\"\"\n",
        "        Update the Q-table using a batch of transitions, considering terminal states represented as -1.\n",
        "\n",
        "        Parameters:\n",
        "        - states: Array of current states.\n",
        "        - actions: Array of actions taken.\n",
        "        - rewards: Array of observed rewards.\n",
        "        - next_states: Array of resulting next states, with -1 for terminal states.\n",
        "        \"\"\"\n",
        "        td_targets = rewards.copy()\n",
        "\n",
        "        # Mask for non-terminal states\n",
        "        non_terminal_mask = next_states != -1\n",
        "\n",
        "        # For non-terminal states, include the discounted future rewards\n",
        "        if np.any(non_terminal_mask):\n",
        "            next_states_non_terminal = next_states[non_terminal_mask].astype(int)  # Convert valid states to int\n",
        "            best_next_actions = np.argmax(self.q_table[next_states_non_terminal], axis=1)\n",
        "            td_targets[non_terminal_mask] += (\n",
        "                self.discount_factor * self.q_table[next_states_non_terminal, best_next_actions]\n",
        "            )\n",
        "\n",
        "        # Ensure td_targets dtype matches q_table dtype\n",
        "        td_targets = td_targets.astype(self.q_table.dtype)\n",
        "\n",
        "        # Update Q-values\n",
        "        for i in range(len(states)):\n",
        "            td_error = td_targets[i] - self.q_table[states[i], actions[i]]\n",
        "            self.q_table[states[i], actions[i]] += self.learning_rate * td_error\n",
        "\n",
        "# Define state and action space sizes\n",
        "state_space_size = trajectory_data_emb['state'].max() + 1\n",
        "action_space_size = trajectory_data_emb['action'].max() + 1\n",
        "\n",
        "# Initialize Q-learning agent\n",
        "agent_emb = QLearningAgent(state_space_size, action_space_size)\n",
        "\n",
        "# Training with batch processing\n",
        "batch_size = 2\n",
        "for i in range(0, len(trajectory_data_emb), batch_size):\n",
        "    # Extract batch\n",
        "    batch = trajectory_data_emb.iloc[i:i + batch_size]\n",
        "    states = batch['state'].values\n",
        "    actions = batch['action'].values\n",
        "    rewards = batch['reward'].astype(float).values\n",
        "    next_states = batch['next_state'].values\n",
        "\n",
        "    # Update Q-table using the batch\n",
        "    agent_emb.update_batch(states, actions, rewards, next_states)\n",
        "\n",
        "# Print the trained Q-table\n",
        "print(\"Trained Q-Table:\")\n",
        "print(agent_emb.q_table)\n"
      ],
      "metadata": {
        "id": "ESmQZFqDC5Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as mcolors\n",
        "# Append optimal actions from Q-table to trajectory data\n",
        "output_data = trajectory_data.copy()\n",
        "output_data['optimal_action'] = trajectory_data['state'].apply(lambda s: np.argmax(agent.q_table[s]))\n",
        "output_data['optimal_action_emb'] = trajectory_data_emb['state'].apply(lambda s: np.argmax(agent_emb.q_table[s]))\n",
        "\n",
        "\n",
        "# 3D Bar Plot for Action Frequency Based on Actions Column\n",
        "action_counts = np.bincount(output_data['action'], minlength=9)\n",
        "action_counts = action_counts / sum(action_counts)\n",
        "\n",
        "optimal_action_counts = np.bincount(output_data['optimal_action'], minlength=9)\n",
        "optimal_action_counts = optimal_action_counts / sum(optimal_action_counts)\n",
        "\n",
        "optimal_action_emb_counts = np.bincount(output_data['optimal_action_emb'], minlength=9)\n",
        "optimal_action_emb_counts = optimal_action_emb_counts / sum(optimal_action_emb_counts)\n",
        "\n",
        "print(action_counts)\n",
        "z_bar_height = max(optimal_action_counts.max(), action_counts.max(), optimal_action_emb_counts.max())\n",
        "\n",
        "# fuild * 3 + vaso\n",
        "# Prepare a 3x3 grid for actions\n",
        "\n",
        "action_grid_x, action_grid_y = np.meshgrid(range(3), range(3))\n",
        "action_grid_z = action_counts.reshape(3, 3)\n",
        "optimal_action_grid_z = optimal_action_counts.reshape(3, 3)\n",
        "optimal_emb_action_grid_z = optimal_action_emb_counts.reshape(3, 3)\n",
        "print(action_grid_z)\n",
        "\n",
        "# Create the 3D bar plot\n",
        "fig = plt.figure(figsize=(20, 14))\n",
        "ax1 = fig.add_subplot(131, projection='3d')\n",
        "ax2 = fig.add_subplot(132, projection='3d')\n",
        "ax3 = fig.add_subplot(133, projection='3d')\n",
        "\n",
        "xpos = action_grid_x.ravel()\n",
        "ypos = action_grid_y.ravel()\n",
        "zpos = np.zeros_like(xpos)\n",
        "\n",
        "\n",
        "\n",
        "for ax, bar_heights, title in [\n",
        "    (ax1, action_grid_z, 'Action Frequency Based on\\n Clinicians Actions'),\n",
        "    (ax2, optimal_action_grid_z, 'Action Frequency Based on\\n Q-Learning Actions'),\n",
        "    (ax3, optimal_emb_action_grid_z, 'Action Frequency Based on\\n Embedding\\nQ-Learning Actions')\n",
        "]:\n",
        "    dx = dy = 0.8  # Width of the bars\n",
        "    dz = bar_heights.flatten()  # Heights of the bars\n",
        "\n",
        "    # Normalize the bar heights (dz) for the colormap\n",
        "    norm = mcolors.Normalize(vmin=dz.min(), vmax=dz.max())\n",
        "    cmap = cm.viridis  # Choose a colormap\n",
        "\n",
        "    # Map the bar heights (dz) to colors\n",
        "    colors = cmap(norm(dz))\n",
        "\n",
        "    ax.bar3d(xpos, ypos, zpos, dx, dy, dz, shade=True,  color=colors, edgecolor='white')\n",
        "    #ax.view_init(elev=30, roll=0)\n",
        "    ax.set_box_aspect(aspect=(1,1,1), zoom=0.9)\n",
        "    ax.set_xlabel('Vasopressor dose        ', labelpad=10)\n",
        "    ax.set_ylabel('       Intravenous\\n          fluids dose', labelpad=30)\n",
        "    ax.set_xticks([0.5, 1.5, 2.5], state_mapping[\"vasopressor\"])\n",
        "    ax.tick_params(axis='y', pad=10)  # Add padding for x-axis\n",
        "    ax.set_yticks([0.5, 1.5, 2.5], state_mapping[\"fluid\"])\n",
        "\n",
        "    ax.set_zlabel('Action %', labelpad=20)\n",
        "    ax.tick_params(axis='z', pad=10)  # Add padding for x-axis\n",
        "    ax.set_zlim(0, z_bar_height)  # Replace max_height with your desired maximum height\n",
        "\n",
        "    ax.set_title(title)\n",
        "\n",
        "    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])  # Required for colorbar\n",
        "    cbar = plt.colorbar(sm, ax=ax, shrink=0.4, pad=0.2)  # Adjust size and position\n",
        "    cbar.set_label('Bar Height', labelpad=10)\n",
        "\n",
        "\n",
        "ax.dist = 20\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oA8GG-HeC5Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate transition probabilities\n",
        "transition_counts = trajectory_data.groupby([\"state\", \"action\", \"next_state\"]).size().reset_index(name=\"count\")\n",
        "state_action_counts = trajectory_data.groupby([\"state\", \"action\"]).size().reset_index(name=\"total_count\")\n",
        "transition_probs = pd.merge(transition_counts, state_action_counts, on=[\"state\", \"action\"])\n",
        "transition_probs[\"probability\"] = transition_probs[\"count\"] / transition_probs[\"total_count\"]\n",
        "\n",
        "# Calculate rewards\n",
        "reward_means = trajectory_data.groupby([\"state\", \"action\"])[\"reward\"].mean().reset_index(name=\"expected_reward\")\n",
        "\n",
        "# Combine transition probabilities and rewards\n",
        "empirical_mdp = pd.merge(transition_probs, reward_means, on=[\"state\", \"action\"])\n"
      ],
      "metadata": {
        "id": "5k0uw42zw-uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clinician Policy"
      ],
      "metadata": {
        "id": "sxaiDLZ6yK7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many times each action was taken for each state\n",
        "state_action_counts = trajectory_data.groupby([\"state\", \"action\"]).size().reset_index(name=\"count\")\n",
        "\n",
        "# Count total number of actions for each state\n",
        "total_state_actions = trajectory_data.groupby(\"state\")[\"action\"].size().reset_index(name=\"total_count\")\n",
        "\n",
        "# Merge counts and calculate probabilities\n",
        "clinician_policy_df = pd.merge(state_action_counts, total_state_actions, on=\"state\")\n",
        "clinician_policy_df[\"probability\"] = clinician_policy_df[\"count\"] / clinician_policy_df[\"total_count\"]\n"
      ],
      "metadata": {
        "id": "Vt7gVYUTh8OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clinician_policy_df.shape"
      ],
      "metadata": {
        "id": "I1y1W3-ya-pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.q_table.shape"
      ],
      "metadata": {
        "id": "XtoqbDHcbVtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "iY-s1tnu5Hdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract learned policy from Q-table\n",
        "learned_policy = np.argmax(agent.q_table, axis=1)\n",
        "\n",
        "learned_policy_emb = np.argmax(agent_emb.q_table, axis=1)"
      ],
      "metadata": {
        "id": "vVkPE9wFsQvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_policy(policy, mdp, num_trajectories=100, max_steps=100):\n",
        "    total_rewards = []\n",
        "\n",
        "    for _ in range(num_trajectories):\n",
        "        state = np.random.choice(mdp['state'].unique())  # Start from a random initial state\n",
        "        state = int(state)  # Ensure state is an integer\n",
        "        cumulative_reward = 0\n",
        "        steps = 0\n",
        "        done = False\n",
        "\n",
        "        while not done and steps < max_steps:\n",
        "            # Select action based on the policy\n",
        "            if isinstance(policy, np.ndarray):  # Learned policy\n",
        "                if state < 0 or state >= len(policy):\n",
        "                    break  # Handle out-of-bounds state\n",
        "                action = policy[state]\n",
        "            elif callable(policy):  # Callable policy (clinician)\n",
        "                action = policy(state)\n",
        "\n",
        "            # Get possible transitions\n",
        "            transitions = mdp[(mdp['state'] == state) & (mdp['action'] == action)]\n",
        "\n",
        "            if transitions.empty:\n",
        "                break\n",
        "\n",
        "            # Sample next state based on transition probabilities\n",
        "            next_state = transitions.sample(weights='probability')['next_state'].values[0]\n",
        "            reward = transitions['expected_reward'].values[0]\n",
        "\n",
        "            cumulative_reward += reward\n",
        "            state = int(next_state) if not pd.isna(next_state) else None  # Ensure valid integer or terminal state\n",
        "\n",
        "            # Check if terminal\n",
        "            done = next_state is None\n",
        "            steps += 1\n",
        "\n",
        "        total_rewards.append(cumulative_reward)\n",
        "\n",
        "    return np.mean(total_rewards), np.std(total_rewards)\n"
      ],
      "metadata": {
        "id": "1ixCKcgcwPKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward, std_reward = simulate_policy(learned_policy, empirical_mdp)\n",
        "print(f\"Learned Policy: Mean Reward = {mean_reward}, Std Dev = {std_reward}\")\n"
      ],
      "metadata": {
        "id": "_eOjC6Ldy5CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward, std_reward = simulate_policy(learned_policy_emb, empirical_mdp)\n",
        "print(f\"Learned Policy: Mean Reward = {mean_reward}, Std Dev = {std_reward}\")"
      ],
      "metadata": {
        "id": "FIyqKcd6VE-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clinician_policy(state):\n",
        "    actions = clinician_policy_df[clinician_policy_df['state'] == state]\n",
        "    return actions.sample(weights='probability')['action'].values[0]\n",
        "\n",
        "mean_reward_clinician, std_reward_clinician = simulate_policy(clinician_policy, empirical_mdp)\n",
        "print(f\"Clinician Policy: Mean Reward = {mean_reward_clinician}, Std Dev = {std_reward_clinician}\")\n"
      ],
      "metadata": {
        "id": "qBofDzi3y83g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Alignment Matrix"
      ],
      "metadata": {
        "id": "9TgKOT5qGqT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have these variables:\n",
        "# - clinician_actions: a list of actions (0-8) taken by clinicians for each state in the trajectory\n",
        "# - model_actions: a list of actions (0-8) predicted by the RL model for the same states\n",
        "# Ensure both are aligned (same length, matching states)\n",
        "\n",
        "# Example dummy data for illustration (replace this with real data)\n",
        "np.random.seed(42)\n",
        "num_actions = 9\n",
        "num_states = 400\n",
        "\n",
        "clinician_action_probs = (\n",
        "    clinician_policy_df.groupby(\"state\")\n",
        "    .apply(lambda group: group.assign(prob=group[\"count\"] / group[\"count\"].sum()))\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Create a dictionary for fast lookup: {state: most_probable_action}\n",
        "clinician_prob_dict = (\n",
        "    clinician_action_probs.groupby(\"state\")\n",
        "    .apply(lambda group: group.loc[group[\"prob\"].idxmax(), \"action\"])  # Most probable action\n",
        "    .to_dict()\n",
        ")\n",
        "# Define a function to compute softmax probabilities from Q-values\n",
        "def softmax(q_values, temperature=1.0):\n",
        "    exp_q = np.exp(q_values / temperature)\n",
        "    return exp_q / np.sum(exp_q)\n",
        "\n",
        "# Create a 9x9 matrix for the heatmap\n",
        "heatmap_matrix = np.zeros((num_actions, num_actions))\n",
        "\n",
        "# Iterate over all states\n",
        "# Iterate over all states\n",
        "for state in range(num_states):\n",
        "    # Most probable clinician action\n",
        "    clinician_action = clinician_prob_dict.get(state, None)\n",
        "    if clinician_action is None:  # Skip if clinician data for the state is missing\n",
        "        continue\n",
        "\n",
        "    # Most probable RL model action\n",
        "    model_probs = softmax(agent_emb.q_table[state])  # Softmax-normalized probabilities\n",
        "    model_action = np.argmax(model_probs)\n",
        "\n",
        "    # Increment the heatmap cell for (clinician_action, model_action)\n",
        "    heatmap_matrix[clinician_action, model_action] += 1\n",
        "\n",
        "# Normalize the heatmap rows so each row sums to 1\n",
        "row_sums = heatmap_matrix.sum(axis=1, keepdims=True)  # Row sums\n",
        "normalized_heatmap_matrix = heatmap_matrix / np.maximum(row_sums, 1)  # Avoid division by zero\n",
        "\n",
        "# Create a DataFrame for the normalized heatmap\n",
        "heatmap_df = pd.DataFrame(\n",
        "    normalized_heatmap_matrix,\n",
        "    index=[f\"Clinician {i}\" for i in range(num_actions)],\n",
        "    columns=[f\"Model {i}\" for i in range(num_actions)]\n",
        ")\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    heatmap_df,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap=\"coolwarm\",\n",
        "    cbar=True,\n",
        "    linewidths=0.5,\n",
        "    linecolor=\"gray\"\n",
        ")\n",
        "plt.title(\"Heatmap: Most Probable Actions (Clinician vs RL Model)\")\n",
        "plt.xlabel(\"RL Model Most Probable Actions\")\n",
        "plt.ylabel(\"Clinician Most Probable Actions\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "bllj5fLPGtyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scope-rl"
      ],
      "metadata": {
        "id": "dD4CDFdSfJVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implement data collection procedure on the RTBGym environment\n",
        "\n",
        "# import SCOPE-RL modules\n",
        "from scope_rl.dataset import SyntheticDataset\n",
        "from scope_rl.policy import EpsilonGreedyHead\n",
        "# import d3rlpy algorithms\n",
        "from d3rlpy.algos import DoubleDQNConfig\n",
        "from d3rlpy.dataset import create_fifo_replay_buffer\n",
        "from d3rlpy.algos import ConstantEpsilonGreedy\n",
        "# import rtbgym and gym\n",
        "import rtbgym\n",
        "import gym\n",
        "import torch\n",
        "# random state\n",
        "random_state = 42\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# (0) Setup environment\n",
        "env = None\n"
      ],
      "metadata": {
        "id": "vanQhXckpanY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_longest_sequence(df):\n",
        "    # Create a sequence ID based on cumulative sum of \"done\"\n",
        "    df[\"sequence_id\"] = df[\"done\"].cumsum()\n",
        "\n",
        "    # Compute the length of each sequence (including the `done=1` row)\n",
        "    sequence_lengths = df.groupby(\"sequence_id\").apply(lambda x: len(x))\n",
        "\n",
        "    # Return the longest sequence length\n",
        "    return sequence_lengths.max()\n",
        "\n",
        "# Compute the longest sequence length\n",
        "longest_sequence = compute_longest_sequence(trajectory_data.copy())\n",
        "print(f\"longest sequence: {longest_sequence}\")\n",
        "\n",
        "\n",
        "def pad_sequences_to_max_length(df):\n",
        "    \"\"\"\n",
        "    Pads all sequences to the length of the longest sequence by adding NaN rows.\n",
        "    \"\"\"\n",
        "    # Create sequence IDs\n",
        "    df[\"sequence_id\"] = df[\"done\"].cumsum()\n",
        "\n",
        "    # Compute sequence lengths\n",
        "    sequence_lengths = df.groupby(\"sequence_id\").apply(lambda x: len(x))\n",
        "\n",
        "    # Determine the maximum sequence length\n",
        "    max_length = sequence_lengths.max()\n",
        "\n",
        "    # Create a list to store padded sequences\n",
        "    padded_sequences = []\n",
        "\n",
        "    # Process each sequence separately\n",
        "    for _, group in df.groupby(\"sequence_id\"):\n",
        "        num_padding_rows = max_length - len(group)\n",
        "        if num_padding_rows > 0:\n",
        "            # Create NaN padding rows\n",
        "            padding_rows = pd.DataFrame(\n",
        "                np.nan,\n",
        "                index=range(num_padding_rows),\n",
        "                columns=df.columns\n",
        "            )\n",
        "            padding_rows[\"done\"] = 1.0\n",
        "            padding_rows[\"action\"] = 0\n",
        "            padding_rows[\"state\"] = 0\n",
        "            padding_rows[\"sequence_id\"] = group[\"sequence_id\"].iloc[0]  # Keep sequence ID\n",
        "            # Append the padded group\n",
        "            padded_sequences.append(pd.concat([group, padding_rows], ignore_index=True))\n",
        "        else:\n",
        "            padded_sequences.append(group)\n",
        "\n",
        "    # Concatenate all padded sequences\n",
        "    padded_df = pd.concat(padded_sequences, ignore_index=True)\n",
        "    return padded_df\n",
        "\n",
        "# Apply the function to pad sequences to the maximum length\n",
        "padded_df = pad_sequences_to_max_length(trajectory_data.copy())\n",
        "padded_df[\"action\"] = padded_df[\"action\"].astype(int)\n",
        "padded_df[\"state\"] = padded_df[\"state\"].astype(int)\n"
      ],
      "metadata": {
        "id": "_Ubol2urqSLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(padded_df))"
      ],
      "metadata": {
        "id": "BpplwkI-qyZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_probs_dist = np.zeros(shape=(len(padded_df), 9))\n",
        "action_probs = np.zeros(shape=(len(padded_df),))\n",
        "\n",
        "for row in tqdm(range(len(padded_df))):\n",
        "    state = padded_df[\"state\"].iloc[row]\n",
        "    taken_action = padded_df[\"action\"].iloc[row]\n",
        "\n",
        "    if pd.isna(state):\n",
        "        continue\n",
        "    state = int(state)\n",
        "\n",
        "    # if we have an action with prob != 0 assign it!\n",
        "    prob = clinician_policy_df[(clinician_policy_df[\"state\"] == state) & (clinician_policy_df[\"action\"] == taken_action)][\"probability\"]\n",
        "    if len(prob) == 1:\n",
        "        action_probs[row] = taken_action\n",
        "    for action in range(9):\n",
        "        state_action_probs = clinician_policy_df[(clinician_policy_df[\"state\"] == state) & (clinician_policy_df[\"action\"] == action)][\"probability\"]\n",
        "        if len(state_action_probs) == 1:\n",
        "            action_probs_dist[row, action] = state_action_probs.iloc[0]\n",
        "        elif len(state_action_probs) == 0:\n",
        "            pass\n",
        "        else:\n",
        "            print(f\"Caution, having double prob value for state: {state} action: {action}\")\n"
      ],
      "metadata": {
        "id": "0A6gNwt1saRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_df.iloc[0][\"state\"]"
      ],
      "metadata": {
        "id": "GYuWve22vd5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clinician_policy_df[clinician_policy_df[\"state\"]==397.0]"
      ],
      "metadata": {
        "id": "W7VQYzApuboe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_probs_dist[0,:]"
      ],
      "metadata": {
        "id": "FnyxL0sRuGIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert action probs to probabilities for action\n",
        "states = np.array(padded_df[\"state\"].to_list())\n",
        "actions = np.array(padded_df[\"action\"].astype(int).to_list())\n",
        "action_prob_arr = action_probs[:, actions]"
      ],
      "metadata": {
        "id": "_xmigfrX3nph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logged_dataset = {\n",
        "    \"size\": len(padded_df),\n",
        "    \"n_trajectories\": int(len(padded_df)/longest_sequence),\n",
        "    \"step_per_trajectory\": longest_sequence,\n",
        "    \"action_type\": \"discrete\",\n",
        "    \"n_actions\": 9,\n",
        "    \"action_dim\": 1,\n",
        "    \"action_meaning\": None,\n",
        "    \"action_keys\": None,\n",
        "    \"state_dim\": 1,\n",
        "    \"state_keys\": None,\n",
        "    \"state\": np.expand_dims(states, -1),\n",
        "    \"action\": np.array(padded_df[\"action\"].astype(int).to_list()),\n",
        "    \"reward\": np.array(padded_df[\"reward\"].to_list()),\n",
        "    \"done\": np.array(padded_df[\"done\"].to_list()),\n",
        "    \"terminal\": np.array(padded_df[\"done\"].to_list()),\n",
        "    \"info\": None,\n",
        "    \"pscore\": action_probs,\n",
        "    \"behavior_policy\": \"clinican_policy\",\n",
        "     \"dataset_id\": 0,\n",
        "}"
      ],
      "metadata": {
        "id": "H6X9gKUvp4Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implement offline RL procedure using scope_rl and d3rlpy\n",
        "\n",
        "# import d3rlpy algorithms\n",
        "from d3rlpy.dataset import MDPDataset\n",
        "from d3rlpy.algos import DiscreteCQLConfig, DiscreteCQL\n",
        "\n",
        "# (3) Learning a new policy from offline logged data (using d3rlpy)\n",
        "# convert dataset into d3rlpy's dataset\n",
        "offlinerl_dataset = MDPDataset(\n",
        "    observations=logged_dataset[\"state\"],\n",
        "    actions=logged_dataset[\"action\"],\n",
        "    rewards=logged_dataset[\"reward\"],\n",
        "    terminals=logged_dataset[\"done\"],\n",
        ")\n",
        "# initialize the algorithm\n",
        "cql = DiscreteCQLConfig().create(device=device)\n",
        "# train an offline policy\n",
        "cql.fit(\n",
        "    offlinerl_dataset,\n",
        "    n_steps=10000,\n",
        ")"
      ],
      "metadata": {
        "id": "S3Uv8OsbpTAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implement OPE procedure using SCOPE-RL\n",
        "\n",
        "# import SCOPE-RL modules\n",
        "from scope_rl.ope import CreateOPEInput\n",
        "from scope_rl.ope import OffPolicyEvaluation as OPE\n",
        "from scope_rl.ope.discrete import DirectMethod as DM\n",
        "from scope_rl.ope.discrete import TrajectoryWiseImportanceSampling as TIS\n",
        "from scope_rl.ope.discrete import PerDecisionImportanceSampling as PDIS\n",
        "from scope_rl.ope.discrete import DoublyRobust as DR\n",
        "\n",
        "\n",
        "random_ = EpsilonGreedyHead(\n",
        "    base_policy=cql,\n",
        "    n_actions=9,\n",
        "    name=\"random\",\n",
        "    epsilon=1.0,\n",
        "    random_state=random_state,\n",
        ")\n",
        "\n",
        "clinican_ = EpsilonGreedyHead(\n",
        "    base_policy=cql,\n",
        "    n_actions=9,\n",
        "    name=\"random\",\n",
        "    epsilon=1.0,\n",
        "    random_state=random_state,\n",
        ")\n",
        "\n",
        "evaluation_policies = [random_, clinican_]\n",
        "# create input for OPE class\n",
        "prep = CreateOPEInput(\n",
        "    env=env,\n",
        ")\n",
        "input_dict = prep.obtain_whole_inputs(\n",
        "    logged_dataset=logged_dataset,\n",
        "    evaluation_policies=evaluation_policies,\n",
        "    require_value_prediction=True,\n",
        "    n_trajectories_on_policy_evaluation=100,\n",
        "    random_state=random_state,\n",
        ")\n",
        "# initialize the OPE class\n",
        "ope = OPE(\n",
        "    logged_dataset=logged_dataset,\n",
        "    ope_estimators=[DM(), TIS(), PDIS(), DR()],\n",
        ")\n",
        "# conduct OPE and visualize the result\n",
        "ope.visualize_off_policy_estimates(\n",
        "    input_dict,\n",
        "    random_state=random_state,\n",
        "    sharey=True,\n",
        ")"
      ],
      "metadata": {
        "id": "UHUwN4RqebXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYxXg7QeesAk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}